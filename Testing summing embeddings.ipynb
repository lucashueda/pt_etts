{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from matplotlib import rcParams \n",
    "rcParams[\"figure.figsize\"] = (16,5)\n",
    "sys.path.append('')\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from TTS.tts.models.tacotron2 import Tacotron2 \n",
    "from TTS.tts.utils import *\n",
    "from TTS.tts.utils.generic_utils import setup_model\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.utils.io import load_config\n",
    "from TTS.tts.utils.text import text_to_sequence, phoneme_to_sequence\n",
    "from TTS.tts.utils.text.symbols import symbols, phonemes\n",
    "import torch\n",
    "from TTS.tts.utils.synthesis import synthesis\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > num_mels:80\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:50.0\n",
      " | > mel_fmax:7600.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > stats_path:\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Using model: Tacotron2\n",
      "Training with 3 speakers and 2 styles\n",
      "Use style target = False\n",
      "Use semi supervised = False\n"
     ]
    }
   ],
   "source": [
    "# Set constants\n",
    "\n",
    "# MODEL_PATH = '/content/drive/MyDrive/Mestrado/TTS/GST_rosana_only/checkpoint_120000.pth.tar'\n",
    "CONFIG_PATH =  './experiments/debug_add_emb/config.json'\n",
    "\n",
    "# MODEL_PATH ='/content/drive/MyDrive/Mestrado/TTS/GST_3speaker_CPQD/best_model.pth.tar'\n",
    "# CONFIG_PATH = '/content/drive/MyDrive/Mestrado/TTS/GST_3speaker_CPQD/config.json'\n",
    "\n",
    "CONFIG = load_config(CONFIG_PATH)\n",
    "# CONFIG['datasets'][0]['path'] = './'\n",
    "# CONFIG['output_path'] = './'\n",
    "# CONFIG['audio']['signal_norm'] = False\n",
    "# CONFIG['audio']['stats_path'] = ''\n",
    "# CONFIG['use_phonemes'] = False\n",
    "# CONFIG['save_step'] = 500\n",
    "CONFIG['num_prosodic_features'] = 0\n",
    "CONFIG['agg_style_space'] = False\n",
    "CONFIG['lookup_speaker_dim'] = 512\n",
    "CONFIG['use_style_lookup'] = False\n",
    "CONFIG['lookup_style_dim'] = 64\n",
    "CONFIG['use_prosodic_linear'] = False\n",
    "CONFIG['prosodic_dim'] = 2\n",
    "use_cuda = False\n",
    "\n",
    "# load the model\n",
    "ap = AudioProcessor(**CONFIG.audio)\n",
    "\n",
    "num_chars = len(phonemes) if CONFIG.use_phonemes else len(symbols)\n",
    "\n",
    "\n",
    "# load the model\n",
    "num_chars = len(phonemes) if CONFIG.use_phonemes else len(symbols)\n",
    "model = setup_model(num_chars, 3, 2, CONFIG, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tacotron2(\n",
      "  (speaker_embedding): Embedding(3, 512)\n",
      "  (embedding): Embedding(217, 512, padding_idx=0)\n",
      "  (encoder): Encoder(\n",
      "    (convolutions): ModuleList(\n",
      "      (0): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (1): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (2): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (lstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (prenet): Prenet(\n",
      "      (linear_layers): ModuleList(\n",
      "        (0): Linear(\n",
      "          (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): Linear(\n",
      "          (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (attention_rnn): LSTMCell(1792, 1024)\n",
      "    (attention): OriginalAttention(\n",
      "      (query_layer): Linear(\n",
      "        (linear_layer): Linear(in_features=1024, out_features=128, bias=False)\n",
      "      )\n",
      "      (inputs_layer): Linear(\n",
      "        (linear_layer): Linear(in_features=1536, out_features=128, bias=False)\n",
      "      )\n",
      "      (v): Linear(\n",
      "        (linear_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "      )\n",
      "      (location_layer): LocationLayer(\n",
      "        (location_conv1d): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
      "        (location_dense): Linear(\n",
      "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder_rnn): LSTMCell(2560, 1024)\n",
      "    (linear_projection): Linear(\n",
      "      (linear_layer): Linear(in_features=2560, out_features=560, bias=True)\n",
      "    )\n",
      "    (stopnet): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(\n",
      "        (linear_layer): Linear(in_features=1584, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (postnet): Postnet(\n",
      "    (convolutions): ModuleList(\n",
      "      (0): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (1): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (2): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (3): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (4): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gst_layer): GST(\n",
      "    (encoder): ReferenceEncoder(\n",
      "      (convs): ModuleList(\n",
      "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (recurrence): GRU(256, 512, batch_first=True)\n",
      "    )\n",
      "  )\n",
      "  (coarse_decoder): Decoder(\n",
      "    (prenet): Prenet(\n",
      "      (linear_layers): ModuleList(\n",
      "        (0): Linear(\n",
      "          (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): Linear(\n",
      "          (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (attention_rnn): LSTMCell(1792, 1024)\n",
      "    (attention): OriginalAttention(\n",
      "      (query_layer): Linear(\n",
      "        (linear_layer): Linear(in_features=1024, out_features=128, bias=False)\n",
      "      )\n",
      "      (inputs_layer): Linear(\n",
      "        (linear_layer): Linear(in_features=1536, out_features=128, bias=False)\n",
      "      )\n",
      "      (v): Linear(\n",
      "        (linear_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "      )\n",
      "      (location_layer): LocationLayer(\n",
      "        (location_conv1d): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
      "        (location_dense): Linear(\n",
      "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder_rnn): LSTMCell(2560, 1024)\n",
      "    (linear_projection): Linear(\n",
      "      (linear_layer): Linear(in_features=2560, out_features=560, bias=True)\n",
      "    )\n",
      "    (stopnet): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(\n",
      "        (linear_layer): Linear(in_features=1584, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TTS.tts.models.tacotron_abstract.TacotronAbstract._add_speaker_embedding(outputs, speaker_embeddings)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._add_speaker_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "s = \"ambulantes vendem os ingressos para a copa do mundo de dois mil e catorze , mas só alguns dias antes do dia do sorteio .\"\n",
    "\n",
    "def text_to_seqvec(text, CONFIG):\n",
    "    text_cleaner = [CONFIG.text_cleaner]\n",
    "    # text ot phonemes to sequence vector\n",
    "    if CONFIG.use_phonemes:\n",
    "        seq = np.asarray(\n",
    "            phoneme_to_sequence(text, text_cleaner, CONFIG.phoneme_language,\n",
    "                                CONFIG.enable_eos_bos_chars,\n",
    "                                tp=CONFIG.characters if 'characters' in CONFIG.keys() else None),\n",
    "            dtype=np.int32)\n",
    "    else:\n",
    "        seq = np.asarray(text_to_sequence(text, text_cleaner, tp=CONFIG.characters if 'characters' in CONFIG.keys() else None), dtype=np.int32)\n",
    "    return seq\n",
    "\n",
    "\n",
    "def numpy_to_torch(np_array, dtype, cuda=False):\n",
    "    if np_array is None:\n",
    "        return None\n",
    "    tensor = torch.as_tensor(np_array, dtype=dtype)\n",
    "    if cuda:\n",
    "        return tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "def id_to_torch(speaker_id, cuda=False):\n",
    "    if speaker_id is not None:\n",
    "        speaker_id = np.asarray(speaker_id)\n",
    "        speaker_id = torch.from_numpy(speaker_id).unsqueeze(0)\n",
    "    if cuda:\n",
    "        return speaker_id.cuda().type(torch.long)\n",
    "    return speaker_id.type(torch.long)\n",
    "\n",
    "def compute_style_mel(style_wav, ap, cuda=False):\n",
    "    style_mel = torch.FloatTensor(ap.melspectrogram(\n",
    "        ap.load_wav(style_wav, sr=ap.sample_rate))).unsqueeze(0)\n",
    "    if cuda:\n",
    "        return style_mel.cuda()\n",
    "    return style_mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id = id_to_torch(2, cuda=use_cuda)\n",
    "\n",
    "inputs = text_to_seqvec(s, CONFIG)\n",
    "inputs = numpy_to_torch(inputs, torch.long, cuda=use_cuda)\n",
    "inputs = inputs.unsqueeze(0)\n",
    "\n",
    "x = model.embedding(inputs).transpose(1, 2)\n",
    "x = model.encoder.inference(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered None\n"
     ]
    }
   ],
   "source": [
    "speaker_ids = speaker_id\n",
    "gst = True\n",
    "num_speakers = 3\n",
    "style_mel = None\n",
    "\n",
    "if gst:\n",
    "    # B x gst_dim\n",
    "    inputs, encoder_outputs, logits = model.compute_gst(x,\n",
    "                                       style_mel,\n",
    "                                       speaker_embeddings if model.gst_use_speaker_embedding else None, style_agg = 'add')\n",
    "# if num_speakers > 1:\n",
    "#     if not model.embeddings_per_sample:\n",
    "#         speaker_embeddings = model.speaker_embedding(speaker_ids)[:, None]\n",
    "#     encoder_outputs = model._concat_speaker_embedding(encoder_outputs, speaker_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = torch.rand(1, 1, 512)\n",
    "model._add_speaker_embedding(encoder_outputs, rand).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4]) torch.Size([1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 2., 5., 2.]]]), torch.Size([1, 1, 4]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetor_A = [1,1,1,1]\n",
    "vetor_B = [0,1,4,1]\n",
    "\n",
    "vetor_A = torch.Tensor(vetor_A)\n",
    "vetor_B = torch.Tensor(vetor_B)\n",
    "\n",
    "vetor_A = vetor_A.unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "vetor_B = vetor_B.unsqueeze(0)\n",
    "\n",
    "print(vetor_A.shape, vetor_B.shape)\n",
    "\n",
    "model._add_speaker_embedding(vetor_A, vetor_B),model._add_speaker_embedding(vetor_A, vetor_B).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12, out=None)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "F.normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5000, 0.5000, 0.5000, 0.5000]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.normalize(vetor_A, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2357, 0.9428, 0.2357]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.normalize(vetor_B, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using CUDA:  True\n",
      " > Number of GPUs:  1\n",
      "\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > num_mels:80\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:50.0\n",
      " | > mel_fmax:7600.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > stats_path:\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " | > Found 10 files in D:\\Mestrado\\Emotion Audio Synthesis (TTS)\\repo_final\\pt_etts\\experiments\\debug_linear_logits\n",
      "Training with 2 speakers: marco, rosana\n",
      "Training with 2 styles + neutral: happy, neutral\n",
      " > Using model: Tacotron2\n",
      "Training with 2 speakers and 2 styles\n",
      "Use style target = False\n",
      "Use semi supervised = False\n",
      "\n",
      " > Model has 60796308 parameters\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/3\u001b[0m\n",
      "\n",
      " > Number of output frames: 7\n",
      "\n",
      " > DataLoader initialization\n",
      " | > Use phonemes: False\n",
      " | > Number of instances : 10\n",
      " | > Max length sequence: 23\n",
      " | > Min length sequence: 2\n",
      " | > Avg length sequence: 13.1\n",
      " | > Num. instances discarded by max-min (max=153, min=6) seq limits: 1\n",
      " | > Batch group size: 8.\n",
      "\n",
      "\u001b[1m > TRAINING (2021-06-21 02:26:48) \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"TTS/bin/train_tts.py\", line 888, in <module>\n",
      "    main(args)\n",
      "  File \"TTS/bin/train_tts.py\", line 756, in main\n",
      "    global_step, epoch, amp, speaker_mapping, style_mapping)\n",
      "  File \"TTS/bin/train_tts.py\", line 202, in train\n",
      "    for num_iter, data in enumerate(data_loader):\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 352, in __iter__\n",
      "    return self._get_iterator()\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 294, in _get_iterator\n",
      "    return _MultiProcessingDataLoaderIter(self)\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 801, in __init__\n",
      "    w.start()\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\multiprocessing\\process.py\", line 105, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\multiprocessing\\context.py\", line 223, in _Popen\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\multiprocessing\\context.py\", line 322, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\multiprocessing\\popen_spawn_win32.py\", line 65, in __init__\n",
      "    reduction.dump(process_obj, to_child)\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\multiprocessing\\spawn.py\", line 105, in spawn_main\n",
      "    exitcode = _main(fd)\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\multiprocessing\\spawn.py\", line 114, in _main\n",
      "    prepare(preparation_data)\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\multiprocessing\\spawn.py\", line 225, in prepare\n",
      "    _fixup_main_from_path(data['init_main_from_path'])\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\multiprocessing\\spawn.py\", line 277, in _fixup_main_from_path\n",
      "    run_name=\"__mp_main__\")\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\runpy.py\", line 263, in run_path\n",
      "    pkg_name=pkg_name, script_name=fname)\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\runpy.py\", line 96, in _run_module_code\n",
      "    mod_name, mod_spec, pkg_name, script_name)\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"D:\\Mestrado\\Emotion Audio Synthesis (TTS)\\repo_final\\pt_etts\\TTS\\bin\\train_tts.py\", line 18, in <module>\n",
      "    import torch\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\torch\\__init__.py\", line 117, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] O arquivo de paginação é muito pequeno para que esta operação seja concluída. Error loading \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.\n"
     ]
    }
   ],
   "source": [
    "! python TTS/bin/train_tts.py --config_path \"./experiments/debug_add_emb/config.json\" --experiment_folder \"./experiments/debug_add_emb/\" --continue_path \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m_audio",
   "language": "python",
   "name": "m_audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
