{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from matplotlib import rcParams \n",
    "rcParams[\"figure.figsize\"] = (16,5)\n",
    "sys.path.append('')\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from TTS.tts.models.tacotron2 import Tacotron2 \n",
    "from TTS.tts.utils import *\n",
    "from TTS.tts.utils.generic_utils import setup_model\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.utils.io import load_config\n",
    "from TTS.tts.utils.text import text_to_sequence, phoneme_to_sequence\n",
    "from TTS.tts.utils.text.symbols import symbols, phonemes\n",
    "import torch\n",
    "from TTS.tts.utils.synthesis import synthesis\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > num_mels:80\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:50.0\n",
      " | > mel_fmax:7600.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > stats_path:\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Using model: Tacotron2\n",
      "Training with 3 speakers and 2 styles\n",
      "Use style target = False\n",
      "Use semi supervised = False\n"
     ]
    }
   ],
   "source": [
    "# Set constants\n",
    "\n",
    "# MODEL_PATH = '/content/drive/MyDrive/Mestrado/TTS/GST_rosana_only/checkpoint_120000.pth.tar'\n",
    "CONFIG_PATH =  './experiments/debug_add_emb/config.json'\n",
    "\n",
    "# MODEL_PATH ='/content/drive/MyDrive/Mestrado/TTS/GST_3speaker_CPQD/best_model.pth.tar'\n",
    "# CONFIG_PATH = '/content/drive/MyDrive/Mestrado/TTS/GST_3speaker_CPQD/config.json'\n",
    "\n",
    "CONFIG = load_config(CONFIG_PATH)\n",
    "# CONFIG['datasets'][0]['path'] = './'\n",
    "# CONFIG['output_path'] = './'\n",
    "# CONFIG['audio']['signal_norm'] = False\n",
    "# CONFIG['audio']['stats_path'] = ''\n",
    "# CONFIG['use_phonemes'] = False\n",
    "# CONFIG['save_step'] = 500\n",
    "CONFIG['num_prosodic_features'] = 0\n",
    "CONFIG['agg_style_space'] = False\n",
    "CONFIG['lookup_speaker_dim'] = 512\n",
    "CONFIG['use_style_lookup'] = False\n",
    "CONFIG['lookup_style_dim'] = 64\n",
    "CONFIG['use_prosodic_linear'] = False\n",
    "CONFIG['prosodic_dim'] = 2\n",
    "use_cuda = False\n",
    "\n",
    "# load the model\n",
    "ap = AudioProcessor(**CONFIG.audio)\n",
    "\n",
    "num_chars = len(phonemes) if CONFIG.use_phonemes else len(symbols)\n",
    "\n",
    "\n",
    "# load the model\n",
    "num_chars = len(phonemes) if CONFIG.use_phonemes else len(symbols)\n",
    "model = setup_model(num_chars, 3, 2, CONFIG, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tacotron2(\n",
      "  (speaker_embedding): Embedding(3, 512)\n",
      "  (embedding): Embedding(217, 512, padding_idx=0)\n",
      "  (encoder): Encoder(\n",
      "    (convolutions): ModuleList(\n",
      "      (0): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (1): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (2): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (lstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (prenet): Prenet(\n",
      "      (linear_layers): ModuleList(\n",
      "        (0): Linear(\n",
      "          (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): Linear(\n",
      "          (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (attention_rnn): LSTMCell(1792, 1024)\n",
      "    (attention): OriginalAttention(\n",
      "      (query_layer): Linear(\n",
      "        (linear_layer): Linear(in_features=1024, out_features=128, bias=False)\n",
      "      )\n",
      "      (inputs_layer): Linear(\n",
      "        (linear_layer): Linear(in_features=1536, out_features=128, bias=False)\n",
      "      )\n",
      "      (v): Linear(\n",
      "        (linear_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "      )\n",
      "      (location_layer): LocationLayer(\n",
      "        (location_conv1d): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
      "        (location_dense): Linear(\n",
      "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder_rnn): LSTMCell(2560, 1024)\n",
      "    (linear_projection): Linear(\n",
      "      (linear_layer): Linear(in_features=2560, out_features=560, bias=True)\n",
      "    )\n",
      "    (stopnet): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(\n",
      "        (linear_layer): Linear(in_features=1584, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (postnet): Postnet(\n",
      "    (convolutions): ModuleList(\n",
      "      (0): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (1): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (2): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (3): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (4): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gst_layer): GST(\n",
      "    (encoder): ReferenceEncoder(\n",
      "      (convs): ModuleList(\n",
      "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (recurrence): GRU(256, 512, batch_first=True)\n",
      "    )\n",
      "  )\n",
      "  (coarse_decoder): Decoder(\n",
      "    (prenet): Prenet(\n",
      "      (linear_layers): ModuleList(\n",
      "        (0): Linear(\n",
      "          (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): Linear(\n",
      "          (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (attention_rnn): LSTMCell(1792, 1024)\n",
      "    (attention): OriginalAttention(\n",
      "      (query_layer): Linear(\n",
      "        (linear_layer): Linear(in_features=1024, out_features=128, bias=False)\n",
      "      )\n",
      "      (inputs_layer): Linear(\n",
      "        (linear_layer): Linear(in_features=1536, out_features=128, bias=False)\n",
      "      )\n",
      "      (v): Linear(\n",
      "        (linear_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "      )\n",
      "      (location_layer): LocationLayer(\n",
      "        (location_conv1d): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
      "        (location_dense): Linear(\n",
      "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder_rnn): LSTMCell(2560, 1024)\n",
      "    (linear_projection): Linear(\n",
      "      (linear_layer): Linear(in_features=2560, out_features=560, bias=True)\n",
      "    )\n",
      "    (stopnet): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(\n",
      "        (linear_layer): Linear(in_features=1584, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TTS.tts.models.tacotron_abstract.TacotronAbstract._add_speaker_embedding(outputs, speaker_embeddings)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._add_speaker_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "s = \"ambulantes vendem os ingressos para a copa do mundo de dois mil e catorze , mas só alguns dias antes do dia do sorteio .\"\n",
    "\n",
    "def text_to_seqvec(text, CONFIG):\n",
    "    text_cleaner = [CONFIG.text_cleaner]\n",
    "    # text ot phonemes to sequence vector\n",
    "    if CONFIG.use_phonemes:\n",
    "        seq = np.asarray(\n",
    "            phoneme_to_sequence(text, text_cleaner, CONFIG.phoneme_language,\n",
    "                                CONFIG.enable_eos_bos_chars,\n",
    "                                tp=CONFIG.characters if 'characters' in CONFIG.keys() else None),\n",
    "            dtype=np.int32)\n",
    "    else:\n",
    "        seq = np.asarray(text_to_sequence(text, text_cleaner, tp=CONFIG.characters if 'characters' in CONFIG.keys() else None), dtype=np.int32)\n",
    "    return seq\n",
    "\n",
    "\n",
    "def numpy_to_torch(np_array, dtype, cuda=False):\n",
    "    if np_array is None:\n",
    "        return None\n",
    "    tensor = torch.as_tensor(np_array, dtype=dtype)\n",
    "    if cuda:\n",
    "        return tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "def id_to_torch(speaker_id, cuda=False):\n",
    "    if speaker_id is not None:\n",
    "        speaker_id = np.asarray(speaker_id)\n",
    "        speaker_id = torch.from_numpy(speaker_id).unsqueeze(0)\n",
    "    if cuda:\n",
    "        return speaker_id.cuda().type(torch.long)\n",
    "    return speaker_id.type(torch.long)\n",
    "\n",
    "def compute_style_mel(style_wav, ap, cuda=False):\n",
    "    style_mel = torch.FloatTensor(ap.melspectrogram(\n",
    "        ap.load_wav(style_wav, sr=ap.sample_rate))).unsqueeze(0)\n",
    "    if cuda:\n",
    "        return style_mel.cuda()\n",
    "    return style_mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id = id_to_torch(2, cuda=use_cuda)\n",
    "\n",
    "inputs = text_to_seqvec(s, CONFIG)\n",
    "inputs = numpy_to_torch(inputs, torch.long, cuda=use_cuda)\n",
    "inputs = inputs.unsqueeze(0)\n",
    "\n",
    "x = model.embedding(inputs).transpose(1, 2)\n",
    "x = model.encoder.inference(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered None\n"
     ]
    }
   ],
   "source": [
    "x = model.inference(inputs, speaker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oi\n"
     ]
    }
   ],
   "source": [
    "if 1: \n",
    "    print('oi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.2762,  0.3342, -0.2501,  ...,  0.0106, -0.0244, -0.2606],\n",
       "          [-0.0091,  0.0212, -0.0278,  ...,  0.2481, -0.1136, -0.1236],\n",
       "          [-0.1327,  0.0782, -0.2828,  ...,  0.3306,  0.0007,  0.1216],\n",
       "          ...,\n",
       "          [-0.1022, -0.0079,  0.0350,  ...,  0.0443, -0.0281, -0.1475],\n",
       "          [-0.1101, -0.2710, -0.2979,  ...,  0.4224,  0.1402, -0.0630],\n",
       "          [-0.0688,  0.0957, -0.0676,  ..., -0.0589, -0.2888, -0.0222]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[ 2.7623e-01,  3.3417e-01, -2.5013e-01,  ...,  1.6349e+00,\n",
       "           -2.4422e-02, -2.6056e-01],\n",
       "          [-4.6211e+00,  2.1195e-02, -2.4959e-01,  ...,  2.4815e-01,\n",
       "            2.8291e+00, -1.2356e-01],\n",
       "          [-2.2278e+00,  5.0520e-01,  1.6054e+00,  ..., -8.8556e-01,\n",
       "            6.8603e-04,  1.5414e+00],\n",
       "          ...,\n",
       "          [-1.0224e-01,  4.1707e+00,  3.7452e-02,  ...,  4.4334e-02,\n",
       "           -2.8128e-02,  1.6176e+00],\n",
       "          [-5.4382e-01, -9.3915e-01, -2.9789e-01,  ...,  4.2243e-01,\n",
       "            1.4017e-01, -2.0598e+00],\n",
       "          [-6.8827e-02, -1.4771e+00,  4.2326e-01,  ..., -3.7502e-01,\n",
       "            5.1458e-01, -2.2224e-02]]], device='cuda:0'),\n",
       " tensor([[[0.0090, 0.0082, 0.0087, 0.0063, 0.0082, 0.0072, 0.0072, 0.0075,\n",
       "           0.0075, 0.0091, 0.0082, 0.0079, 0.0076, 0.0084, 0.0075, 0.0090,\n",
       "           0.0082, 0.0084, 0.0084, 0.0074, 0.0084, 0.0094, 0.0075, 0.0092,\n",
       "           0.0073, 0.0071, 0.0078, 0.0076, 0.0071, 0.0088, 0.0078, 0.0078,\n",
       "           0.0082, 0.0076, 0.0084, 0.0098, 0.0092, 0.0088, 0.0085, 0.0086,\n",
       "           0.0099, 0.0092, 0.0093, 0.0088, 0.0093, 0.0095, 0.0087, 0.0086,\n",
       "           0.0086, 0.0091, 0.0094, 0.0072, 0.0076, 0.0085, 0.0083, 0.0060,\n",
       "           0.0074, 0.0091, 0.0088, 0.0103, 0.0091, 0.0085, 0.0093, 0.0081,\n",
       "           0.0077, 0.0088, 0.0085, 0.0087, 0.0096, 0.0090, 0.0083, 0.0082,\n",
       "           0.0085, 0.0090, 0.0085, 0.0083, 0.0074, 0.0087, 0.0083, 0.0068,\n",
       "           0.0069, 0.0083, 0.0078, 0.0080, 0.0083, 0.0103, 0.0087, 0.0087,\n",
       "           0.0088, 0.0083, 0.0088, 0.0080, 0.0077, 0.0080, 0.0081, 0.0076,\n",
       "           0.0062, 0.0073, 0.0078, 0.0094, 0.0083, 0.0086, 0.0092, 0.0082,\n",
       "           0.0094, 0.0108, 0.0104, 0.0095, 0.0089, 0.0077, 0.0075, 0.0070,\n",
       "           0.0073, 0.0080, 0.0071, 0.0074, 0.0087, 0.0086, 0.0082, 0.0083],\n",
       "          [0.0091, 0.0082, 0.0087, 0.0063, 0.0082, 0.0072, 0.0072, 0.0075,\n",
       "           0.0075, 0.0091, 0.0082, 0.0080, 0.0076, 0.0084, 0.0075, 0.0090,\n",
       "           0.0081, 0.0083, 0.0084, 0.0074, 0.0083, 0.0094, 0.0075, 0.0092,\n",
       "           0.0073, 0.0070, 0.0078, 0.0076, 0.0070, 0.0088, 0.0078, 0.0078,\n",
       "           0.0082, 0.0076, 0.0085, 0.0098, 0.0092, 0.0088, 0.0085, 0.0086,\n",
       "           0.0100, 0.0092, 0.0093, 0.0089, 0.0093, 0.0095, 0.0087, 0.0086,\n",
       "           0.0086, 0.0091, 0.0095, 0.0072, 0.0076, 0.0084, 0.0082, 0.0060,\n",
       "           0.0073, 0.0091, 0.0088, 0.0103, 0.0091, 0.0084, 0.0093, 0.0081,\n",
       "           0.0077, 0.0088, 0.0086, 0.0088, 0.0096, 0.0089, 0.0084, 0.0082,\n",
       "           0.0086, 0.0090, 0.0086, 0.0083, 0.0074, 0.0087, 0.0083, 0.0069,\n",
       "           0.0069, 0.0083, 0.0078, 0.0080, 0.0083, 0.0103, 0.0087, 0.0087,\n",
       "           0.0088, 0.0083, 0.0088, 0.0080, 0.0077, 0.0081, 0.0082, 0.0075,\n",
       "           0.0062, 0.0073, 0.0078, 0.0094, 0.0082, 0.0086, 0.0092, 0.0082,\n",
       "           0.0095, 0.0109, 0.0104, 0.0095, 0.0089, 0.0077, 0.0075, 0.0070,\n",
       "           0.0073, 0.0080, 0.0071, 0.0073, 0.0086, 0.0086, 0.0081, 0.0083]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[0.5220],\n",
       "          [0.5270]]], device='cuda:0'),\n",
       " None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered None\n"
     ]
    }
   ],
   "source": [
    "speaker_ids = speaker_id\n",
    "gst = True\n",
    "num_speakers = 3\n",
    "style_mel = None\n",
    "\n",
    "if gst:\n",
    "    # B x gst_dim\n",
    "    inputs, encoder_outputs, logits = model.compute_gst(x,\n",
    "                                       style_mel,\n",
    "                                       speaker_embeddings if model.gst_use_speaker_embedding else None, style_agg = 'add')\n",
    "# if num_speakers > 1:\n",
    "#     if not model.embeddings_per_sample:\n",
    "#         speaker_embeddings = model.speaker_embedding(speaker_ids)[:, None]\n",
    "#     encoder_outputs = model._concat_speaker_embedding(encoder_outputs, speaker_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 120, 512])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = torch.rand(1, 1, 512)\n",
    "model._add_speaker_embedding(encoder_outputs, rand).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4]) torch.Size([1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 2., 5., 2.]]]), torch.Size([1, 1, 4]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetor_A = [1,1,1,1]\n",
    "vetor_B = [0,1,4,1]\n",
    "\n",
    "vetor_A = torch.Tensor(vetor_A)\n",
    "vetor_B = torch.Tensor(vetor_B)\n",
    "\n",
    "vetor_A = vetor_A.unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "vetor_B = vetor_B.unsqueeze(0)\n",
    "\n",
    "print(vetor_A.shape, vetor_B.shape)\n",
    "\n",
    "model._add_speaker_embedding(vetor_A, vetor_B),model._add_speaker_embedding(vetor_A, vetor_B).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12, out=None)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "F.normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5000, 0.5000, 0.5000, 0.5000]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.normalize(vetor_A, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2357, 0.9428, 0.2357]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.normalize(vetor_B, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using CUDA:  True\n",
      " > Number of GPUs:  1\n",
      "\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > num_mels:80\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:50.0\n",
      " | > mel_fmax:7600.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > stats_path:\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " | > Found 10 files in D:\\Mestrado\\Emotion Audio Synthesis (TTS)\\repo_final\\pt_etts\\experiments\\debug_linear_logits\n",
      "Training with 2 speakers: marco, rosana\n",
      "Training with 2 styles + neutral: happy, neutral\n",
      " > Using model: Tacotron2\n",
      "Training with 2 speakers and 2 styles\n",
      "Use style target = False\n",
      "Use semi supervised = False\n",
      "\n",
      " > Model has 60796308 parameters\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/3\u001b[0m\n",
      "\n",
      " > Number of output frames: 7\n",
      "\n",
      " > DataLoader initialization\n",
      " | > Use phonemes: False\n",
      " | > Number of instances : 10\n",
      " | > Max length sequence: 23\n",
      " | > Min length sequence: 2\n",
      " | > Avg length sequence: 13.1\n",
      " | > Num. instances discarded by max-min (max=153, min=6) seq limits: 1\n",
      " | > Batch group size: 8.\n",
      "\n",
      "\u001b[1m > TRAINING (2021-06-21 02:26:48) \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"TTS/bin/train_tts.py\", line 888, in <module>\n",
      "    main(args)\n",
      "  File \"TTS/bin/train_tts.py\", line 756, in main\n",
      "    global_step, epoch, amp, speaker_mapping, style_mapping)\n",
      "  File \"TTS/bin/train_tts.py\", line 202, in train\n",
      "    for num_iter, data in enumerate(data_loader):\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 352, in __iter__\n",
      "    return self._get_iterator()\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 294, in _get_iterator\n",
      "    return _MultiProcessingDataLoaderIter(self)\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 801, in __init__\n",
      "    w.start()\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\multiprocessing\\process.py\", line 105, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\multiprocessing\\context.py\", line 223, in _Popen\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\multiprocessing\\context.py\", line 322, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\multiprocessing\\popen_spawn_win32.py\", line 65, in __init__\n",
      "    reduction.dump(process_obj, to_child)\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\multiprocessing\\spawn.py\", line 105, in spawn_main\n",
      "    exitcode = _main(fd)\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\multiprocessing\\spawn.py\", line 114, in _main\n",
      "    prepare(preparation_data)\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\multiprocessing\\spawn.py\", line 225, in prepare\n",
      "    _fixup_main_from_path(data['init_main_from_path'])\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\multiprocessing\\spawn.py\", line 277, in _fixup_main_from_path\n",
      "    run_name=\"__mp_main__\")\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\runpy.py\", line 263, in run_path\n",
      "    pkg_name=pkg_name, script_name=fname)\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\runpy.py\", line 96, in _run_module_code\n",
      "    mod_name, mod_spec, pkg_name, script_name)\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"D:\\Mestrado\\Emotion Audio Synthesis (TTS)\\repo_final\\pt_etts\\TTS\\bin\\train_tts.py\", line 18, in <module>\n",
      "    import torch\n",
      "  File \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\torch\\__init__.py\", line 117, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] O arquivo de paginação é muito pequeno para que esta operação seja concluída. Error loading \"C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.\n"
     ]
    }
   ],
   "source": [
    "! python TTS/bin/train_tts.py --config_path \"./experiments/debug_add_emb/config.json\" --experiment_folder \"./experiments/debug_add_emb/\" --continue_path \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['phoneme_cleaners']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaner = [CONFIG.text_cleaner]\n",
    "text_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = '''Um professor, para se tornar doutor, investe, pelo menos, 8 anos de sua vida ( se partir da graduação direto pro doutorado). Se fizer graduação, mestrado e doutorado, são 10 anos. Se fizer graduação, especialização, mestrado e doutorado, lá se vão 12 anos. Sim, 12 anos de universidade. Todo esse tempo labutando com ensino, pesquisa e extensão. Produzindo, apresentando e publicando trabalhos.  Participando de debates e congressos. Dando aula. Ministrando cursos. Participando de bancas. Emitindo pareceres. Orientando artigos, projetos de conclusão de cursos, TCC's e etc, etc, etc. De repente, todo esse conhecimento arduamente produzido dentro dos parâmetros científicos e acadêmicos passa a não valer  nada ante a palavra de um esquizóide qualquer que define a forma como nós, professores, devemos conduzir o trabalho para o qual nos preparamos ao longo da vida e diuturnamente - pesquisando e estudando métodos e teorias, produzindo e submetendo nossas ideias à comunidade científica - que, todos sabemos, não é muito conhecida pela generosidade. Sem comentar os diversos perrengues -  familiares, socioemocionais, financeiros e de saúde -  que enfrentamos durante e  em virtude dessa caminhada extenuante e tão mal recompensada. Ainda assim, contra tudo e todos, persistimos cantando no mal tempo, porque temos a certeza daquilo que fazemos, e assentamos nossa autoridade e dignidade no conhecimento que produzimos. \n",
    "# Antes, a violência que se abatia sobre nós, profissionais da educação, era apenas financeira e a desvalorização social era decorrente dessa desvalorização financeira: '' Você é só uma professorazinha, ganha pouco''. \n",
    "# Agora, a violência é pior: a palavra e a autoridade intelectual do professor -  que ainda mantínhamos, apesar de tudo -  são jogados por terra. \n",
    "# Somos confrontados por pessoas que não leem, não têm conhecimento de absolutamente nada e que se informam apenas por youtubers, líderes religiosos delirantes ou figuras obscuras sem nenhum (re)conhecimento científico e que são  merecidamente proscritas do meio acadêmico.  Ainda por cima, somos chamados de '' doutrinadores'', acusados e vilipendiados das piores formas possíveis sem que os difamadores se retratem ou sofram as devidas punições por suas afirmações criminosas que têm como intuito deslegitimar, mais uma vez, a profissão docente nos violentando moralmente. E, como se não bastasse, há ''professores'' que se alinham a esse  evidente projeto de destruição da educação no país. Não me admira que a profissão esteja entre as menos procuradas pelos jovens, e que esteja, infelizmente, caminhando para a extinção.'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "s = \"ambulantes vendem os ingressos para a copa do mundo de dois mil e catorze , mas só alguns dias antes do dia do sorteio .\"\n",
    "\n",
    "# s = \"e é\"\n",
    "\n",
    "seq = np.asarray(text_to_sequence(s, text_cleaner), dtype=np.int32)\n",
    "chars_var = torch.from_numpy(seq).unsqueeze(0).cuda()\n",
    "\n",
    "# style_wav = {'1': 0.3} # Parece um token interessante\n",
    "\n",
    "# f = 0.1\n",
    "# style_wav = {}\n",
    "# n_tokens = 3\n",
    "# for i in range(n_tokens):\n",
    "#     style_wav[str(i)] = f\n",
    "\n",
    "style_wav = './data/LJSpeech/LJSpeech-1.1/wavs/LJ001-0003.wav'\n",
    "\n",
    "speaker_embedding = None\n",
    "speaker_id = 2\n",
    "\n",
    "\n",
    "# style_wav = None\n",
    "\n",
    "use_cuda = True\n",
    "\n",
    "wav, alignment, decoder_output, postnet_output, stop_tokens, _, logits = synthesis(\n",
    "    model.cuda(),\n",
    "    s,\n",
    "    CONFIG,\n",
    "    use_cuda,\n",
    "    ap,\n",
    "    speaker_id=speaker_id,\n",
    "    speaker_embedding=speaker_embedding,\n",
    "    style_wav=style_wav,\n",
    "    truncated=False,\n",
    "    enable_eos_bos_chars=CONFIG.enable_eos_bos_chars, #pylint: disable=unused-argument\n",
    "    use_griffin_lim=True,\n",
    "    do_trim_silence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5120,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m_audio",
   "language": "python",
   "name": "m_audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
