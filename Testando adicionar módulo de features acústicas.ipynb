{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyworld as pw\n",
    "import librosa\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ch_df = pd.read_csv(\"D:/Mestrado/CH Corpus/CH_Unicamp-20210309T013712Z-003/CH_Unicamp/base_recortes/recortes_silvano/correspondencias.csv\", sep = ';',\n",
    "                    names=[\"orig_name\", \"file_name\", \"style\", \"un1\", \"un2\",\"un3\", 'un4'])\n",
    "\n",
    "emotions_corresp = pd.read_csv(\"D:/Mestrado/CH Corpus/CH_Unicamp-20210309T013712Z-003/CH_Unicamp/base_recortes/recortes_silvano/emotions.csv\", sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_emotions = dict(emotions_corresp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_df['style'] = ch_df['style'].map(map_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_neutral = []\n",
    "for orig_name in ch_df.orig_name:\n",
    "    if re.search('Neutra', orig_name):\n",
    "        is_neutral.append(1)\n",
    "    else:\n",
    "        is_neutral.append(0)\n",
    "  \n",
    "ch_df['is_neutral'] = is_neutral\n",
    "ch_df.loc[ch_df['is_neutral'] == 1, 'style'] = 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitch_range(audio_path, sr = None, pmin = 5, pmax = 95):\n",
    "    '''\n",
    "        Takes the audio (.wav) file path and return the pitch range.\n",
    "        \n",
    "        Here, pitch range is defined as follows in https://arxiv.org/pdf/2009.06775v1.pdf\n",
    "        \n",
    "        pitch range = P95(pitch) - P05(pitch), where pitch is the pitch contour ignoring silence.\n",
    "        \n",
    "    '''\n",
    "    x, fs = librosa.load(audio_path, sr=sr)\n",
    "    _f0, t = pw.dio(x.astype(np.double), fs)    # raw pitch extractor\n",
    "    f0 = pw.stonemask(x.astype(np.double), _f0, t, fs)  # pitch refinement\n",
    "    \n",
    "    lower_bound = np.percentile(f0[f0>0], pmin)\n",
    "    upper_bound = np.percentile(f0[f0>0], pmax)\n",
    "    \n",
    "    pitch_range = upper_bound - lower_bound\n",
    "    \n",
    "    return pitch_range\n",
    "\n",
    "def get_logpitch_mean(audio_path, sr = None):\n",
    "    '''\n",
    "        Takes the audio (.wav) file path and return the pitch mean.\n",
    "        \n",
    "        Here, log pitch mean is defined as follows in https://arxiv.org/pdf/2009.06775v1.pdf\n",
    "        \n",
    "        log pitch mean = mean(log(pitch)), where pitch is the pitch contour ignoring silence.\n",
    "        \n",
    "    '''\n",
    "    x, fs = librosa.load(audio_path, sr=sr)\n",
    "    _f0, t = pw.dio(x.astype(np.double), fs)    # raw pitch extractor\n",
    "    f0 = pw.stonemask(x.astype(np.double), _f0, t, fs)  # pitch refinement\n",
    "    \n",
    "    logpitch_mean = np.log(f0[f0>0]).mean()\n",
    "    \n",
    "    return logpitch_mean\n",
    "\n",
    "def get_energy(audio_path, sr = None, top_level_db = 10, frame_length=1024, hop_length = 512):\n",
    "    '''\n",
    "        Takes the audio (.wav) file path and return the mean speech energy.\n",
    "        \n",
    "        Here, speech energy is defined as follows in https://arxiv.org/pdf/2009.06775v1.pdf\n",
    "        \n",
    "        E = 20*log(mean(abs(x))), where x is audio amplitudes without silence\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    x, fs = librosa.load(audio_path, sr=sr)\n",
    "    \n",
    "    # Getting the non silent partitions\n",
    "    non_silent_partitions = librosa.effects.split(x, top_db=top_level_db, frame_length=frame_length, hop_length=hop_length)\n",
    "    x_clean = []\n",
    "    for interval in non_silent_partitions:\n",
    "        x_clean.extend(x[interval[0]:interval[1]])\n",
    "\n",
    "    x_clean = np.array(x_clean) \n",
    "    \n",
    "    energy = 20*np.log(abs(x).mean())\n",
    "    \n",
    "    return energy\n",
    "\n",
    "def get_cpqd_lab_speaking_rate(audio_file, lab_path):\n",
    "    '''\n",
    "        Takes the audio (.wav) file path and lab (.lab) file path from CPqD environment.\n",
    "        \n",
    "        It counts the phones/duration. Which is a proxy for speaking rate.\n",
    "    '''\n",
    "    \n",
    "    x , sr = librosa.load(audio_file, sr = None)\n",
    "    \n",
    "    with open(lab_path , 'r', encoding = 'latin-1') as f:\n",
    "        for k in f.readlines():\n",
    "            if(\"phones\" in k[:10]):\n",
    "                qtde_phones = len(k[10:].replace('|', '').split())\n",
    "                break\n",
    "    \n",
    "    qtde_phones = round(qtde_phones/(len(x)/sr), 3)\n",
    "    \n",
    "    return qtde_phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"D:/Mestrado/CH Corpus/CH_Unicamp-20210309T013712Z-003/CH_Unicamp/base_recortes/recortes_silvano/audios/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchs = []\n",
    "\n",
    "for wav in ch_df['file_name'].values:\n",
    "    wav_path = local_path + wav + '.wav'\n",
    "    pitch_range = get_pitch_range(wav_path)\n",
    "    pitchs.append(pitch_range)\n",
    "    \n",
    "ch_df['pitch_range'] = pitchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pitchs = []\n",
    "\n",
    "for wav in ch_df['file_name'].values:\n",
    "    wav_path = local_path + wav + '.wav'\n",
    "    pitch_mean = get_logpitch_mean(wav_path)\n",
    "    m_pitchs.append(pitch_mean)\n",
    "    \n",
    "ch_df['pitch_logpitch_mean'] = m_pitchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = []\n",
    "\n",
    "for wav in ch_df['file_name'].values:\n",
    "    wav_path = local_path + wav + '.wav'\n",
    "    energy = get_energy(wav_path)\n",
    "    energies.append(energy)\n",
    "    \n",
    "ch_df['energy'] = energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean pitch range by emotion\n",
    "\n",
    "pitch_mean = ch_df.groupby('style').agg({'pitch_range': np.mean}).reset_index()\n",
    "pitch_min = ch_df.groupby('style').agg({'pitch_range': np.min}).reset_index()\n",
    "pitch_max = ch_df.groupby('style').agg({'pitch_range': np.max}).reset_index()\n",
    "\n",
    "pitch_df = pitch_mean\n",
    "pitch_df['pitch_min'] = pitch_min['pitch_range']\n",
    "pitch_df['pitch_max'] = pitch_max['pitch_range']\n",
    "pitch_df.sort_values(by='pitch_range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean pitch range by emotion\n",
    "\n",
    "logpitch_mean = ch_df.groupby('style').agg({'pitch_logpitch_mean': np.mean}).reset_index()\n",
    "logpitch_min = ch_df.groupby('style').agg({'pitch_logpitch_mean': np.min}).reset_index()\n",
    "logpitch_max = ch_df.groupby('style').agg({'pitch_logpitch_mean': np.max}).reset_index()\n",
    "\n",
    "logpitch_df = logpitch_mean\n",
    "logpitch_df['pitch_min'] = logpitch_min['pitch_logpitch_mean']\n",
    "logpitch_df['pitch_max'] = logpitch_max['pitch_logpitch_mean']\n",
    "logpitch_df.sort_values(by='pitch_logpitch_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean pitch range by emotion\n",
    "\n",
    "energy_mean = ch_df.groupby('style').agg({'energy': np.mean}).reset_index()\n",
    "energy_min = ch_df.groupby('style').agg({'energy': np.min}).reset_index()\n",
    "energy_max = ch_df.groupby('style').agg({'energy': np.max}).reset_index()\n",
    "\n",
    "energy_df = energy_mean\n",
    "energy_df['energy_min'] = energy_min['energy']\n",
    "energy_df['energy_max'] = energy_max['energy']\n",
    "energy_df.sort_values(by='energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_df['norm_pitch_range'] = (ch_df['pitch_range'] - ch_df['pitch_range'].mean())/ch_df['pitch_range'].std()\n",
    "ch_df['norm_energy'] = (ch_df['energy'] - ch_df['energy'].mean())/ch_df['energy'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anal_styles = ['Happy-for','Fear']\n",
    "plt.figure(figsize=(10,10))\n",
    "for style in ch_df['style'].unique():\n",
    "    if(style in anal_styles):\n",
    "        filt_df = ch_df[ch_df['style'] == style]\n",
    "        plt.scatter(filt_df['norm_pitch_range'], filt_df['norm_energy'], label = f'Emotion = {style}', alpha = 0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelEncoder()\n",
    "\n",
    "ch_df['target'] = lb.fit_transform(ch_df['style'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonneutral = ch_df[ch_df['style'] != 'Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(df_nonneutral[['norm_pitch_range','norm_energy']], df_nonneutral['target'], test_size = 0.2,\n",
    "                                                 stratify = df_nonneutral['target'], random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state = 42)\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x_train, y_train)\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = local_path + 'silvano0002.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_local_path = \"D:/Mestrado/CH Corpus/CH_Unicamp-20210309T013712Z-003/CH_Unicamp/base_recortes/recortes_silvano/cpqd_transcripts/\"\n",
    "lab_path = lab_local_path + 'silvano0002.lab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cpqd_lab_speaking_rate(audio_path, lab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = local_path + 'silvano0001.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_energy(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = 'D:/Mestrado/CH Corpus/CH_Unicamp-20210309T013712Z-003/CH_Unicamp/base_recortes/recortes_silvano/audios/silvano0001.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pitch_range(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_silent_partitions = librosa.effects.split(x, top_db=10, frame_length=1024, hop_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_silent_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_clean = []\n",
    "for interval in non_silent_partitions:\n",
    "    x_clean.extend(x[interval[0]:interval[1]])\n",
    "\n",
    "x_clean = np.array(x_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20*np.log(x_clean.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[non_silent_partitions[0][0]:non_silent_partitions[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x[non_silent_partitions[0][0]:non_silent_partitions[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, fs = librosa.load('/content/drive/MyDrive/CH_Unicamp/base_recortes/recortes_silvano/audios/silvano0002.wav', sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_f0, t = pw.dio(x.astype(np.double), fs)    # raw pitch extractor\n",
    "f0 = pw.stonemask(x.astype(np.double), _f0, t, fs)  # pitch refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from random import randrange\n",
    "from torch.utils.data import DataLoader\n",
    "from TTS.tts.datasets.preprocess import load_meta_data\n",
    "from TTS.tts.datasets.TTSDataset import MyDataset\n",
    "from TTS.tts.layers.losses import TacotronLoss\n",
    "from TTS.tts.utils.distribute import (DistributedSampler,\n",
    "                                      apply_gradient_allreduce,\n",
    "                                      init_distributed, reduce_tensor)\n",
    "from TTS.tts.utils.generic_utils import setup_model, check_config_tts\n",
    "from TTS.tts.utils.io import save_best_model, save_checkpoint\n",
    "from TTS.tts.utils.measures import alignment_diagonal_score\n",
    "from TTS.tts.utils.speakers import (get_speakers, load_speaker_mapping,\n",
    "                                    save_speaker_mapping)\n",
    "from TTS.tts.utils.styles import (get_styles, load_style_mapping,\n",
    "                                    save_style_mapping)\n",
    "from TTS.tts.utils.synthesis import synthesis\n",
    "from TTS.tts.utils.text.symbols import make_symbols, phonemes, symbols\n",
    "from TTS.tts.utils.visual import plot_alignment, plot_spectrogram\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.utils.console_logger import ConsoleLogger\n",
    "from TTS.utils.generic_utils import (KeepAverage, count_parameters,\n",
    "                                     create_experiment_folder, get_git_branch,\n",
    "                                     remove_experiment_folder, set_init_dict)\n",
    "from TTS.utils.io import copy_config_file, load_config\n",
    "from TTS.utils.radam import RAdam\n",
    "from TTS.utils.tensorboard_logger import TensorboardLogger\n",
    "from TTS.utils.training import (NoamLR, adam_weight_decay, check_update,\n",
    "                                gradual_training_scheduler, set_weight_decay,\n",
    "                                setup_torch_training_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using CUDA:  True\n",
      " > Number of GPUs:  1\n"
     ]
    }
   ],
   "source": [
    "use_cuda, num_gpus = setup_torch_training_env(True, False)\n",
    "\n",
    "def setup_loader(ap, r, is_val=False, verbose=False, speaker_mapping=None, style_mapping = None):\n",
    "    if is_val and not c.run_eval:\n",
    "        loader = None\n",
    "    else:\n",
    "        dataset = MyDataset(\n",
    "            r,\n",
    "            c.text_cleaner,\n",
    "            compute_linear_spec=c.model.lower() == 'tacotron',\n",
    "            meta_data=meta_data_eval if is_val else meta_data_train,\n",
    "            ap=ap,\n",
    "            tp=c.characters if 'characters' in c.keys() else None,\n",
    "            batch_group_size=0 if is_val else c.batch_group_size *\n",
    "            c.batch_size,\n",
    "            min_seq_len=c.min_seq_len,\n",
    "            max_seq_len=c.max_seq_len,\n",
    "            phoneme_cache_path=c.phoneme_cache_path,\n",
    "            use_phonemes=c.use_phonemes,\n",
    "            phoneme_language=c.phoneme_language,\n",
    "            enable_eos_bos=c.enable_eos_bos_chars,\n",
    "            verbose=verbose,\n",
    "            speaker_mapping=speaker_mapping if c.use_speaker_embedding and c.use_external_speaker_embedding_file else None)\n",
    "        sampler = DistributedSampler(dataset) if num_gpus > 1 else None\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=c.eval_batch_size if is_val else c.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=dataset.collate_fn,\n",
    "            drop_last=False,\n",
    "            sampler=sampler,\n",
    "            num_workers=c.num_val_loader_workers\n",
    "            if is_val else c.num_loader_workers,\n",
    "            pin_memory=False)\n",
    "    return loader\n",
    "\n",
    "def format_data(data, speaker_mapping=None, style_mapping = None):\n",
    "    if speaker_mapping is None and c.use_speaker_embedding and not c.use_external_speaker_embedding_file:\n",
    "        speaker_mapping = load_speaker_mapping(OUT_PATH)\n",
    "    if style_mapping is None and c.use_style_embedding:\n",
    "        style_mapping = load_style_mapping(OUT_PATH)\n",
    "\n",
    "    # setup input data\n",
    "    text_input = data[0]\n",
    "    text_lengths = data[1]\n",
    "    speaker_names = data[2]\n",
    "    linear_input = data[3] if c.model in [\"Tacotron\"] else None\n",
    "    mel_input = data[4]\n",
    "    mel_lengths = data[5]\n",
    "    stop_targets = data[6]\n",
    "    style_targets = data[10]\n",
    "    pitch_range = data[11]\n",
    "    speaking_rate = data[12]\n",
    "    energy = data[13]\n",
    "\n",
    "    avg_text_length = torch.mean(text_lengths.float())\n",
    "    avg_spec_length = torch.mean(mel_lengths.float())\n",
    "\n",
    "    if c.use_speaker_embedding:\n",
    "        if c.use_external_speaker_embedding_file:\n",
    "            speaker_embeddings = data[8]\n",
    "            speaker_ids = None\n",
    "        else:\n",
    "            speaker_ids = [\n",
    "                speaker_mapping[speaker_name] for speaker_name in speaker_names\n",
    "            ]\n",
    "            speaker_ids = torch.LongTensor(speaker_ids)\n",
    "            speaker_embeddings = None\n",
    "    else:\n",
    "        speaker_embeddings = None\n",
    "        speaker_ids = None\n",
    "\n",
    "    if c.use_style_embedding:\n",
    "        style_targets = [\n",
    "                style_mapping[style_target] for style_target in style_targets\n",
    "            ]\n",
    "        if c.use_one_hot_style: # Style target will be a one hotted vector\n",
    "            style_targets_ = np.zeros((len(style_targets), len(style_mapping)-1))\n",
    "            for i in range(len(style_targets_)):\n",
    "                if(style_targets[i] != 0): # If we force the 0 mapped style to be the non\n",
    "                    style_targets_[i][style_targets[i]-1] = 1 # For each position we one hot encode it\n",
    "            \n",
    "            style_targets = style_targets_\n",
    "            \n",
    "            style_targets = torch.FloatTensor(style_targets)\n",
    "            \n",
    "            del style_targets_\n",
    "        else: # Style target will be just the indice \n",
    "            style_targets = torch.LongTensor(style_targets) # To use in CrossEntropyLoss need to be LongTensor\n",
    "    else:\n",
    "        style_targets = None\n",
    "\n",
    "    # Prosodic features          \n",
    "    if pitch_range is not None:\n",
    "        pitch_range = torch.LongTensor(pitch_range)\n",
    "    \n",
    "    if speaking_rate is not None:\n",
    "        speaking_rate = torch.LongTensor(speaking_rate)\n",
    "\n",
    "    if energy is not None:\n",
    "        energy = torch.LongTensor(energy)\n",
    "\n",
    "\n",
    "    # set stop targets view, we predict a single stop token per iteration.\n",
    "    stop_targets = stop_targets.view(text_input.shape[0],\n",
    "                                     stop_targets.size(1) // c.r, -1)\n",
    "    stop_targets = (stop_targets.sum(2) >\n",
    "                    0.0).unsqueeze(2).float().squeeze(2)\n",
    "\n",
    "    # dispatch data to GPU\n",
    "    if use_cuda:\n",
    "        text_input = text_input.cuda(non_blocking=True)\n",
    "        text_lengths = text_lengths.cuda(non_blocking=True)\n",
    "        mel_input = mel_input.cuda(non_blocking=True)\n",
    "        mel_lengths = mel_lengths.cuda(non_blocking=True)\n",
    "        linear_input = linear_input.cuda(non_blocking=True) if c.model in [\"Tacotron\"] else None\n",
    "        stop_targets = stop_targets.cuda(non_blocking=True)\n",
    "        if speaker_ids is not None:\n",
    "            speaker_ids = speaker_ids.cuda(non_blocking=True)\n",
    "        if speaker_embeddings is not None:\n",
    "            speaker_embeddings = speaker_embeddings.cuda(non_blocking=True)\n",
    "        if style_targets is not None:\n",
    "            style_targets = style_targets.cuda(non_blocking=True)\n",
    "\n",
    "        # Prosodic features          \n",
    "        if pitch_range is not None:\n",
    "            pitch_range = pitch_range.cuda(non_blocking=True)\n",
    "        \n",
    "        if speaking_rate is not None:\n",
    "            speaking_rate = speaking_rate.cuda(non_blocking=True)\n",
    "\n",
    "        if energy is not None:\n",
    "            energy = energy.cuda(non_blocking=True)\n",
    "\n",
    "    return text_input, text_lengths, mel_input, mel_lengths, linear_input, stop_targets, speaker_ids, \\\n",
    "        speaker_embeddings, avg_text_length, avg_spec_length, style_targets, pitch_range, speaking_rate, energy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = './experiments/debug_linear_logits/config.json'\n",
    "experiment_folder = './experiments/debug_linear_logits/'\n",
    "c = load_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = experiment_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > num_mels:80\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:50.0\n",
      " | > mel_fmax:7600.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > stats_path:\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n"
     ]
    }
   ],
   "source": [
    "ap = AudioProcessor(**c.audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Found 10 files in D:\\Mestrado\\Emotion Audio Synthesis (TTS)\\repo_final\\pt_etts\\experiments\\debug_linear_logits\n"
     ]
    }
   ],
   "source": [
    "meta_data_train, meta_data_eval = load_meta_data(c.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'characters' in c.keys():\n",
    "    symbols, phonemes = make_symbols(**c.characters)\n",
    "\n",
    "# DISTRUBUTED\n",
    "if num_gpus > 1:\n",
    "    init_distributed(args.rank, num_gpus, args.group_id,\n",
    "                     c.distributed[\"backend\"], c.distributed[\"url\"])\n",
    "num_chars = len(phonemes) if c.use_phonemes else len(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 2 speakers: marco, rosana\n",
      "Training with 2 styles: happy, neutral\n",
      " > Using model: Tacotron2\n",
      "Training with 2 speakers and 2 styles\n",
      "Use style target = True\n",
      "Use semi supervised = False\n"
     ]
    }
   ],
   "source": [
    "restore_path = None # primeiro ele Ã© none pra criar o json\n",
    "# restore_path = experiment_folder\n",
    "\n",
    "# parse speakers\n",
    "if c.use_speaker_embedding:\n",
    "    speakers = get_speakers(meta_data_train)\n",
    "    if restore_path:\n",
    "        if c.use_external_speaker_embedding_file: # if restore checkpoint and use External Embedding file\n",
    "            prev_out_path = os.path.dirname(args.restore_path)\n",
    "            speaker_mapping = load_speaker_mapping(prev_out_path)\n",
    "            if not speaker_mapping:\n",
    "                print(\"WARNING: speakers.json was not found in restore_path, trying to use CONFIG.external_speaker_embedding_file\")\n",
    "                speaker_mapping = load_speaker_mapping(c.external_speaker_embedding_file)\n",
    "                if not speaker_mapping:\n",
    "                    raise RuntimeError(\"You must copy the file speakers.json to restore_path, or set a valid file in CONFIG.external_speaker_embedding_file\")\n",
    "            speaker_embedding_dim = len(speaker_mapping[list(speaker_mapping.keys())[0]]['embedding'])\n",
    "        elif not c.use_external_speaker_embedding_file: # if restore checkpoint and don't use External Embedding file\n",
    "            prev_out_path = os.path.dirname(restore_path)\n",
    "            speaker_mapping = load_speaker_mapping(prev_out_path)\n",
    "            speaker_embedding_dim = None\n",
    "            assert all([speaker in speaker_mapping\n",
    "                        for speaker in speakers]), \"As of now you, you cannot \" \\\n",
    "                                                \"introduce new speakers to \" \\\n",
    "                                                \"a previously trained model.\"\n",
    "    else: # if start new train and don't use External Embedding file\n",
    "        speaker_mapping = {name: i for i, name in enumerate(speakers)}\n",
    "        speaker_embedding_dim = None\n",
    "    save_speaker_mapping(OUT_PATH, speaker_mapping)\n",
    "    num_speakers = len(speaker_mapping)\n",
    "    print(\"Training with {} speakers: {}\".format(num_speakers,\n",
    "                                                 \", \".join(speakers)))\n",
    "else:\n",
    "    num_speakers = 0\n",
    "    speaker_embedding_dim = None\n",
    "    speaker_mapping = None\n",
    "    \n",
    "    \n",
    "# parse styles\n",
    "if c.use_style_embedding:\n",
    "    styles = get_styles(meta_data_train)\n",
    "    if restore_path:\n",
    "        prev_out_path = os.path.dirname(restore_path)\n",
    "        style_mapping = load_style_mapping(prev_out_path)\n",
    "        style_embedding_dim = None\n",
    "        assert all([style in style_mapping\n",
    "                    for style in styles]), \"As of now you, you cannot \" \\\n",
    "                                            \"introduce new styles to \" \\\n",
    "                                            \"a previously trained model.\"\n",
    "    else: # if start new train and don't use External Embedding file\n",
    "        style_mapping = {name: i for i, name in enumerate(styles)}\n",
    "        style_embedding_dim = None\n",
    "    save_style_mapping(OUT_PATH, style_mapping)\n",
    "    num_styles = len(style_mapping)\n",
    "    print(\"Training with {} styles: {}\".format(num_styles,\n",
    "                                                 \", \".join(styles)))\n",
    "else:\n",
    "    num_styles = 0\n",
    "    style_embedding_dim = None\n",
    "    style_mapping = None\n",
    "\n",
    "model = setup_model(num_chars, num_speakers, num_styles, c, speaker_embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " > DataLoader initialization\n",
      " | > Use phonemes: False\n",
      " | > Number of instances : 10\n",
      " | > Max length sequence: 23\n",
      " | > Min length sequence: 2\n",
      " | > Avg length sequence: 13.1\n",
      " | > Num. instances discarded by max-min (max=153, min=6) seq limits: 1\n",
      " | > Batch group size: 128.\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "data_loader = setup_loader(ap, model.decoder.r, is_val=False,\n",
    "                           verbose=(epoch == 0), speaker_mapping=speaker_mapping, style_mapping = style_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for data in data_loader:\n",
    "    text_input, text_lengths, mel_input, mel_lengths, linear_input, stop_targets, speaker_ids, speaker_embeddings, avg_text_length, avg_spec_length, style_targets, \\\n",
    "    pitch_range, speaking_rate, energy = format_data(data, speaker_mapping, style_mapping)\n",
    "    if c.bidirectional_decoder or c.double_decoder_consistency:\n",
    "        decoder_output, postnet_output, alignments, stop_tokens, decoder_backward_output, alignments_backward, logits = model.cuda()(\n",
    "            text_input, text_lengths, mel_input, mel_lengths, speaker_ids=speaker_ids, speaker_embeddings=speaker_embeddings)\n",
    "    else:\n",
    "        decoder_output, postnet_output, alignments, stop_tokens, logits = model.cuda()(\n",
    "            text_input, text_lengths, mel_input, mel_lengths, speaker_ids=speaker_ids, speaker_embeddings=speaker_embeddings)\n",
    "        decoder_backward_output = None\n",
    "        alignments_backward = None\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 609, 80])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(pitch_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mask, output_mask = model.compute_masks(text_lengths, mel_lengths)\n",
    "# B x D_embed x T_in_max\n",
    "embedded_inputs = model.embedding(text_input).transpose(1, 2)\n",
    "# B x T_in_max x D_en\n",
    "encoder_outputs = model.encoder(embedded_inputs, text_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_out, gst_out, logits = model.compute_gst(encoder_outputs, mel_input, None, True, style_targets,style_targets,style_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9, 23, 1027]), torch.Size([9, 23, 512]), torch.Size([9, 1, 515]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_out.shape, encoder_outputs.shape, gst_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 1, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_targets.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 23, 1025])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._concat_speaker_embedding(enc_out, style_targets.unsqueeze(1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'Tacotron2',\n",
       " 'run_name': 'ljspeech-ddc',\n",
       " 'run_description': 'tacotron2 with DDC and differential spectral loss.',\n",
       " 'audio': {'fft_size': 1024,\n",
       "  'win_length': 1024,\n",
       "  'hop_length': 256,\n",
       "  'frame_length_ms': None,\n",
       "  'frame_shift_ms': None,\n",
       "  'sample_rate': 16000,\n",
       "  'preemphasis': 0.0,\n",
       "  'ref_level_db': 20,\n",
       "  'do_trim_silence': True,\n",
       "  'trim_db': 60,\n",
       "  'power': 1.5,\n",
       "  'griffin_lim_iters': 60,\n",
       "  'num_mels': 80,\n",
       "  'mel_fmin': 50.0,\n",
       "  'mel_fmax': 7600.0,\n",
       "  'spec_gain': 1,\n",
       "  'signal_norm': False,\n",
       "  'min_level_db': -100,\n",
       "  'symmetric_norm': True,\n",
       "  'max_norm': 4.0,\n",
       "  'clip_norm': True,\n",
       "  'stats_path': ''},\n",
       " 'distributed': {'backend': 'nccl', 'url': 'tcp://localhost:54321'},\n",
       " 'reinit_layers': [],\n",
       " 'batch_size': 32,\n",
       " 'eval_batch_size': 1,\n",
       " 'r': 7,\n",
       " 'gradual_training': [[0, 7, 2],\n",
       "  [1, 5, 2],\n",
       "  [50000, 3, 1],\n",
       "  [130000, 2, 32],\n",
       "  [290000, 1, 32]],\n",
       " 'apex_amp_level': None,\n",
       " 'loss_masking': True,\n",
       " 'decoder_loss_alpha': 0.5,\n",
       " 'postnet_loss_alpha': 0.25,\n",
       " 'ga_alpha': 5.0,\n",
       " 'diff_spec_alpha': 0.25,\n",
       " 'gst_style_loss': True,\n",
       " 'run_eval': True,\n",
       " 'test_delay_epochs': 10,\n",
       " 'test_sentences_file': None,\n",
       " 'noam_schedule': False,\n",
       " 'grad_clip': 1.0,\n",
       " 'epochs': 3,\n",
       " 'lr': 0.0001,\n",
       " 'wd': 1e-06,\n",
       " 'warmup_steps': 4000,\n",
       " 'seq_len_norm': False,\n",
       " 'memory_size': -1,\n",
       " 'prenet_type': 'original',\n",
       " 'prenet_dropout': False,\n",
       " 'attention_type': 'original',\n",
       " 'attention_heads': 4,\n",
       " 'attention_norm': 'sigmoid',\n",
       " 'windowing': False,\n",
       " 'use_forward_attn': False,\n",
       " 'forward_attn_mask': False,\n",
       " 'transition_agent': False,\n",
       " 'location_attn': True,\n",
       " 'bidirectional_decoder': False,\n",
       " 'double_decoder_consistency': True,\n",
       " 'ddc_r': 7,\n",
       " 'stopnet': True,\n",
       " 'separate_stopnet': True,\n",
       " 'print_step': 25,\n",
       " 'tb_plot_step': 100,\n",
       " 'print_eval': False,\n",
       " 'save_step': 10000,\n",
       " 'checkpoint': True,\n",
       " 'tb_model_param_stats': False,\n",
       " 'text_cleaner': 'phoneme_cleaners',\n",
       " 'enable_eos_bos_chars': False,\n",
       " 'num_loader_workers': 4,\n",
       " 'num_val_loader_workers': 4,\n",
       " 'batch_group_size': 4,\n",
       " 'min_seq_len': 6,\n",
       " 'max_seq_len': 153,\n",
       " 'output_path': './',\n",
       " 'phoneme_cache_path': '/home/erogol/Models/phoneme_cache/',\n",
       " 'use_phonemes': False,\n",
       " 'phoneme_language': 'en-us',\n",
       " 'use_speaker_embedding': True,\n",
       " 'use_gst': True,\n",
       " 'use_external_speaker_embedding_file': False,\n",
       " 'external_speaker_embedding_file': '../../speakers-vctk-en.json',\n",
       " 'use_style_embedding': True,\n",
       " 'use_one_hot_style': True,\n",
       " 'semi_supervised': False,\n",
       " 'lookup_speaker_dim': 64,\n",
       " 'gst': {'gst_style_input': None,\n",
       "  'gst_embedding_dim': 512,\n",
       "  'gst_num_heads': 1,\n",
       "  'gst_style_tokens': 2,\n",
       "  'gst_use_speaker_embedding': False,\n",
       "  'gst_use_linear_style_target': True,\n",
       "  'use_only_reference': True},\n",
       " 'datasets': [{'name': 'style_reader',\n",
       "   'path': './experiments/debug_linear_logits/',\n",
       "   'meta_file_train': 'debug_meta.csv',\n",
       "   'meta_file_val': 'debug_meta.csv'}]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m_audio",
   "language": "python",
   "name": "m_audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
