{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyworld as pw\n",
    "import librosa\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ch_df = pd.read_csv(\"D:/Mestrado/CH Corpus/CH_Unicamp-20210309T013712Z-003/CH_Unicamp/base_recortes/recortes_silvano/correspondencias.csv\", sep = ';',\n",
    "                    names=[\"orig_name\", \"file_name\", \"style\", \"un1\", \"un2\",\"un3\", 'un4'])\n",
    "\n",
    "emotions_corresp = pd.read_csv(\"D:/Mestrado/CH Corpus/CH_Unicamp-20210309T013712Z-003/CH_Unicamp/base_recortes/recortes_silvano/emotions.csv\", sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_emotions = dict(emotions_corresp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_df['style'] = ch_df['style'].map(map_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_neutral = []\n",
    "for orig_name in ch_df.orig_name:\n",
    "    if re.search('Neutra', orig_name):\n",
    "        is_neutral.append(1)\n",
    "    else:\n",
    "        is_neutral.append(0)\n",
    "  \n",
    "ch_df['is_neutral'] = is_neutral\n",
    "ch_df.loc[ch_df['is_neutral'] == 1, 'style'] = 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_df['style'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitch_range(audio_path, sr = None, pmin = 5, pmax = 95):\n",
    "    '''\n",
    "        Takes the audio (.wav) file path and return the pitch range.\n",
    "        \n",
    "        Here, pitch range is defined as follows in https://arxiv.org/pdf/2009.06775v1.pdf\n",
    "        \n",
    "        pitch range = P95(pitch) - P05(pitch), where pitch is the pitch contour ignoring silence.\n",
    "        \n",
    "    '''\n",
    "    x, fs = librosa.load(audio_path, sr=sr)\n",
    "    _f0, t = pw.dio(x.astype(np.double), fs)    # raw pitch extractor\n",
    "    f0 = pw.stonemask(x.astype(np.double), _f0, t, fs)  # pitch refinement\n",
    "    \n",
    "    lower_bound = np.percentile(f0[f0>0], pmin)\n",
    "    upper_bound = np.percentile(f0[f0>0], pmax)\n",
    "    \n",
    "    pitch_range = upper_bound - lower_bound\n",
    "    \n",
    "    return pitch_range\n",
    "\n",
    "def get_logpitch_mean(audio_path, sr = None):\n",
    "    '''\n",
    "        Takes the audio (.wav) file path and return the pitch mean.\n",
    "        \n",
    "        Here, log pitch mean is defined as follows in https://arxiv.org/pdf/2009.06775v1.pdf\n",
    "        \n",
    "        log pitch mean = mean(log(pitch)), where pitch is the pitch contour ignoring silence.\n",
    "        \n",
    "    '''\n",
    "    x, fs = librosa.load(audio_path, sr=sr)\n",
    "    _f0, t = pw.dio(x.astype(np.double), fs)    # raw pitch extractor\n",
    "    f0 = pw.stonemask(x.astype(np.double), _f0, t, fs)  # pitch refinement\n",
    "    \n",
    "    logpitch_mean = np.log(f0[f0>0]).mean()\n",
    "    \n",
    "    return logpitch_mean\n",
    "\n",
    "def get_energy(audio_path, sr = None, top_level_db = 10, frame_length=1024, hop_length = 512):\n",
    "    '''\n",
    "        Takes the audio (.wav) file path and return the mean speech energy.\n",
    "        \n",
    "        Here, speech energy is defined as follows in https://arxiv.org/pdf/2009.06775v1.pdf\n",
    "        \n",
    "        E = 20*log(mean(abs(x))), where x is audio amplitudes without silence\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    x, fs = librosa.load(audio_path, sr=sr)\n",
    "    \n",
    "    # Getting the non silent partitions\n",
    "    non_silent_partitions = librosa.effects.split(x, top_db=top_level_db, frame_length=frame_length, hop_length=hop_length)\n",
    "    x_clean = []\n",
    "    for interval in non_silent_partitions:\n",
    "        x_clean.extend(x[interval[0]:interval[1]])\n",
    "\n",
    "    x_clean = np.array(x_clean) \n",
    "    \n",
    "    energy = 20*np.log(abs(x).mean())\n",
    "    \n",
    "    return energy\n",
    "\n",
    "def get_cpqd_lab_speaking_rate(audio_file, lab_path):\n",
    "    '''\n",
    "        Takes the audio (.wav) file path and lab (.lab) file path from CPqD environment.\n",
    "        \n",
    "        It counts the phones/duration. Which is a proxy for speaking rate.\n",
    "    '''\n",
    "    \n",
    "    x , sr = librosa.load(audio_file, sr = None)\n",
    "    \n",
    "    with open(lab_path , 'r', encoding = 'latin-1') as f:\n",
    "        for k in f.readlines():\n",
    "            if(\"phones\" in k[:10]):\n",
    "                qtde_phones = len(k[10:].replace('|', '').split())\n",
    "                break\n",
    "    \n",
    "    qtde_phones = round(qtde_phones/(len(x)/sr), 3)\n",
    "    \n",
    "    return qtde_phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"D:/Mestrado/CH Corpus/CH_Unicamp-20210309T013712Z-003/CH_Unicamp/base_recortes/recortes_silvano/audios/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchs = []\n",
    "\n",
    "for wav in ch_df['file_name'].values:\n",
    "    wav_path = local_path + wav + '.wav'\n",
    "    pitch_range = get_pitch_range(wav_path)\n",
    "    pitchs.append(pitch_range)\n",
    "    \n",
    "ch_df['pitch_range'] = pitchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pitchs = []\n",
    "\n",
    "for wav in ch_df['file_name'].values:\n",
    "    wav_path = local_path + wav + '.wav'\n",
    "    pitch_mean = get_logpitch_mean(wav_path)\n",
    "    m_pitchs.append(pitch_mean)\n",
    "    \n",
    "ch_df['pitch_logpitch_mean'] = m_pitchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = []\n",
    "\n",
    "for wav in ch_df['file_name'].values:\n",
    "    wav_path = local_path + wav + '.wav'\n",
    "    energy = get_energy(wav_path)\n",
    "    energies.append(energy)\n",
    "    \n",
    "ch_df['energy'] = energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean pitch range by emotion\n",
    "\n",
    "pitch_mean = ch_df.groupby('style').agg({'pitch_range': np.mean}).reset_index()\n",
    "pitch_min = ch_df.groupby('style').agg({'pitch_range': np.min}).reset_index()\n",
    "pitch_max = ch_df.groupby('style').agg({'pitch_range': np.max}).reset_index()\n",
    "\n",
    "pitch_df = pitch_mean\n",
    "pitch_df['pitch_min'] = pitch_min['pitch_range']\n",
    "pitch_df['pitch_max'] = pitch_max['pitch_range']\n",
    "pitch_df.sort_values(by='pitch_range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean pitch range by emotion\n",
    "\n",
    "logpitch_mean = ch_df.groupby('style').agg({'pitch_logpitch_mean': np.mean}).reset_index()\n",
    "logpitch_min = ch_df.groupby('style').agg({'pitch_logpitch_mean': np.min}).reset_index()\n",
    "logpitch_max = ch_df.groupby('style').agg({'pitch_logpitch_mean': np.max}).reset_index()\n",
    "\n",
    "logpitch_df = logpitch_mean\n",
    "logpitch_df['pitch_min'] = logpitch_min['pitch_logpitch_mean']\n",
    "logpitch_df['pitch_max'] = logpitch_max['pitch_logpitch_mean']\n",
    "logpitch_df.sort_values(by='pitch_logpitch_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean pitch range by emotion\n",
    "\n",
    "energy_mean = ch_df.groupby('style').agg({'energy': np.mean}).reset_index()\n",
    "energy_min = ch_df.groupby('style').agg({'energy': np.min}).reset_index()\n",
    "energy_max = ch_df.groupby('style').agg({'energy': np.max}).reset_index()\n",
    "\n",
    "energy_df = energy_mean\n",
    "energy_df['energy_min'] = energy_min['energy']\n",
    "energy_df['energy_max'] = energy_max['energy']\n",
    "energy_df.sort_values(by='energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_df['norm_pitch_range'] = (ch_df['pitch_range'] - ch_df['pitch_range'].mean())/ch_df['pitch_range'].std()\n",
    "ch_df['norm_energy'] = (ch_df['energy'] - ch_df['energy'].mean())/ch_df['energy'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anal_styles = ['Happy-for','Fear']\n",
    "plt.figure(figsize=(10,10))\n",
    "for style in ch_df['style'].unique():\n",
    "    if(style in anal_styles):\n",
    "        filt_df = ch_df[ch_df['style'] == style]\n",
    "        plt.scatter(filt_df['norm_pitch_range'], filt_df['norm_energy'], label = f'Emotion = {style}', alpha = 0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelEncoder()\n",
    "\n",
    "ch_df['target'] = lb.fit_transform(ch_df['style'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonneutral = ch_df[ch_df['style'] != 'Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(df_nonneutral[['norm_pitch_range','norm_energy']], df_nonneutral['target'], test_size = 0.2,\n",
    "                                                 stratify = df_nonneutral['target'], random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state = 42)\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x_train, y_train)\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = local_path + 'silvano0002.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_local_path = \"D:/Mestrado/CH Corpus/CH_Unicamp-20210309T013712Z-003/CH_Unicamp/base_recortes/recortes_silvano/cpqd_transcripts/\"\n",
    "lab_path = lab_local_path + 'silvano0002.lab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cpqd_lab_speaking_rate(audio_path, lab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = local_path + 'silvano0001.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_energy(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = 'D:/Mestrado/CH Corpus/CH_Unicamp-20210309T013712Z-003/CH_Unicamp/base_recortes/recortes_silvano/audios/silvano0001.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pitch_range(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_silent_partitions = librosa.effects.split(x, top_db=10, frame_length=1024, hop_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_silent_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_clean = []\n",
    "for interval in non_silent_partitions:\n",
    "    x_clean.extend(x[interval[0]:interval[1]])\n",
    "\n",
    "x_clean = np.array(x_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20*np.log(x_clean.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[non_silent_partitions[0][0]:non_silent_partitions[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x[non_silent_partitions[0][0]:non_silent_partitions[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, fs = librosa.load('/content/drive/MyDrive/CH_Unicamp/base_recortes/recortes_silvano/audios/silvano0002.wav', sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_f0, t = pw.dio(x.astype(np.double), fs)    # raw pitch extractor\n",
    "f0 = pw.stonemask(x.astype(np.double), _f0, t, fs)  # pitch refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.read_csv(\"./experiments/debug_gst_logits/debug_meta_prosodic.csv\", delimiter='|', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base['pitch_range'] = 1\n",
    "base['speaking_rate'] = 0.22\n",
    "base['energy'] = 0.2\n",
    "\n",
    "base.to_csv(\"./experiments/debug_gst_logits/debug_meta_prosodic.csv\", index = False, encoding='latin-1', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from random import randrange\n",
    "from torch.utils.data import DataLoader\n",
    "from TTS.tts.datasets.preprocess import load_meta_data\n",
    "from TTS.tts.datasets.TTSDataset import MyDataset\n",
    "from TTS.tts.layers.losses import TacotronLoss\n",
    "from TTS.tts.utils.distribute import (DistributedSampler,\n",
    "                                      apply_gradient_allreduce,\n",
    "                                      init_distributed, reduce_tensor)\n",
    "from TTS.tts.utils.generic_utils import setup_model, check_config_tts\n",
    "from TTS.tts.utils.io import save_best_model, save_checkpoint\n",
    "from TTS.tts.utils.measures import alignment_diagonal_score\n",
    "from TTS.tts.utils.speakers import (get_speakers, load_speaker_mapping,\n",
    "                                    save_speaker_mapping)\n",
    "from TTS.tts.utils.styles import (get_styles, load_style_mapping,\n",
    "                                    save_style_mapping)\n",
    "from TTS.tts.utils.synthesis import synthesis\n",
    "from TTS.tts.utils.text.symbols import make_symbols, phonemes, symbols\n",
    "from TTS.tts.utils.visual import plot_alignment, plot_spectrogram\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.utils.console_logger import ConsoleLogger\n",
    "from TTS.utils.generic_utils import (KeepAverage, count_parameters,\n",
    "                                     create_experiment_folder, get_git_branch,\n",
    "                                     remove_experiment_folder, set_init_dict)\n",
    "from TTS.utils.io import copy_config_file, load_config\n",
    "from TTS.utils.radam import RAdam\n",
    "from TTS.utils.tensorboard_logger import TensorboardLogger\n",
    "from TTS.utils.training import (NoamLR, adam_weight_decay, check_update,\n",
    "                                gradual_training_scheduler, set_weight_decay,\n",
    "                                setup_torch_training_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using CUDA:  True\n",
      " > Number of GPUs:  1\n"
     ]
    }
   ],
   "source": [
    "use_cuda, num_gpus = setup_torch_training_env(True, False)\n",
    "\n",
    "def setup_loader(ap, r, is_val=False, verbose=False, speaker_mapping=None, style_mapping = None):\n",
    "    if is_val and not c.run_eval:\n",
    "        loader = None\n",
    "    else:\n",
    "        dataset = MyDataset(\n",
    "            r,\n",
    "            c.text_cleaner,\n",
    "            compute_linear_spec=c.model.lower() == 'tacotron',\n",
    "            meta_data=meta_data_eval if is_val else meta_data_train,\n",
    "            ap=ap,\n",
    "            tp=c.characters if 'characters' in c.keys() else None,\n",
    "            batch_group_size=0 if is_val else c.batch_group_size *\n",
    "            c.batch_size,\n",
    "            min_seq_len=c.min_seq_len,\n",
    "            max_seq_len=c.max_seq_len,\n",
    "            phoneme_cache_path=c.phoneme_cache_path,\n",
    "            use_phonemes=c.use_phonemes,\n",
    "            phoneme_language=c.phoneme_language,\n",
    "            enable_eos_bos=c.enable_eos_bos_chars,\n",
    "            verbose=verbose,\n",
    "            speaker_mapping=speaker_mapping if c.use_speaker_embedding and c.use_external_speaker_embedding_file else None)\n",
    "        sampler = DistributedSampler(dataset) if num_gpus > 1 else None\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=c.eval_batch_size if is_val else c.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=dataset.collate_fn,\n",
    "            drop_last=False,\n",
    "            sampler=sampler,\n",
    "            num_workers=c.num_val_loader_workers\n",
    "            if is_val else c.num_loader_workers,\n",
    "            pin_memory=False)\n",
    "    return loader\n",
    "\n",
    "def format_data(data, speaker_mapping=None, style_mapping = None):\n",
    "    if speaker_mapping is None and c.use_speaker_embedding and not c.use_external_speaker_embedding_file:\n",
    "        speaker_mapping = load_speaker_mapping(OUT_PATH)\n",
    "    if style_mapping is None and c.use_style_embedding:\n",
    "        style_mapping = load_style_mapping(OUT_PATH)\n",
    "\n",
    "    # setup input data\n",
    "    text_input = data[0]\n",
    "    text_lengths = data[1]\n",
    "    speaker_names = data[2]\n",
    "    linear_input = data[3] if c.model in [\"Tacotron\"] else None\n",
    "    mel_input = data[4]\n",
    "    mel_lengths = data[5]\n",
    "    stop_targets = data[6]\n",
    "    style_targets = data[10]\n",
    "    pitch_range = data[11]\n",
    "    speaking_rate = data[12]\n",
    "    energy = data[13]\n",
    "\n",
    "    avg_text_length = torch.mean(text_lengths.float())\n",
    "    avg_spec_length = torch.mean(mel_lengths.float())\n",
    "\n",
    "    if c.use_speaker_embedding:\n",
    "        if c.use_external_speaker_embedding_file:\n",
    "            speaker_embeddings = data[8]\n",
    "            speaker_ids = None\n",
    "        else:\n",
    "            speaker_ids = [\n",
    "                speaker_mapping[speaker_name] for speaker_name in speaker_names\n",
    "            ]\n",
    "            speaker_ids = torch.LongTensor(speaker_ids)\n",
    "            speaker_embeddings = None\n",
    "    else:\n",
    "        speaker_embeddings = None\n",
    "        speaker_ids = None\n",
    "\n",
    "    if c.use_style_embedding:\n",
    "        style_targets = [\n",
    "                style_mapping[style_target] for style_target in style_targets\n",
    "            ]\n",
    "        if c.use_one_hot_style: # Style target will be a one hotted vector\n",
    "            style_targets_ = np.zeros((len(style_targets), len(style_mapping)-1))\n",
    "            for i in range(len(style_targets_)):\n",
    "                if(style_targets[i] != 0): # If we force the 0 mapped style to be the non\n",
    "                    style_targets_[i][style_targets[i]-1] = 1 # For each position we one hot encode it\n",
    "            \n",
    "            style_targets = style_targets_\n",
    "            \n",
    "            style_targets = torch.FloatTensor(style_targets)\n",
    "            \n",
    "            del style_targets_\n",
    "        else: # Style target will be just the indice \n",
    "            style_targets = torch.LongTensor(style_targets) # To use in CrossEntropyLoss need to be LongTensor\n",
    "    else:\n",
    "        style_targets = None\n",
    "\n",
    "    # Prosodic features          \n",
    "    if pitch_range is not None:\n",
    "        pitch_range = torch.LongTensor(pitch_range)\n",
    "    \n",
    "    if speaking_rate is not None:\n",
    "        speaking_rate = torch.LongTensor(speaking_rate)\n",
    "\n",
    "    if energy is not None:\n",
    "        energy = torch.LongTensor(energy)\n",
    "\n",
    "\n",
    "    # set stop targets view, we predict a single stop token per iteration.\n",
    "    stop_targets = stop_targets.view(text_input.shape[0],\n",
    "                                     stop_targets.size(1) // c.r, -1)\n",
    "    stop_targets = (stop_targets.sum(2) >\n",
    "                    0.0).unsqueeze(2).float().squeeze(2)\n",
    "\n",
    "    # dispatch data to GPU\n",
    "    if use_cuda:\n",
    "        text_input = text_input.cuda(non_blocking=True)\n",
    "        text_lengths = text_lengths.cuda(non_blocking=True)\n",
    "        mel_input = mel_input.cuda(non_blocking=True)\n",
    "        mel_lengths = mel_lengths.cuda(non_blocking=True)\n",
    "        linear_input = linear_input.cuda(non_blocking=True) if c.model in [\"Tacotron\"] else None\n",
    "        stop_targets = stop_targets.cuda(non_blocking=True)\n",
    "        if speaker_ids is not None:\n",
    "            speaker_ids = speaker_ids.cuda(non_blocking=True)\n",
    "        if speaker_embeddings is not None:\n",
    "            speaker_embeddings = speaker_embeddings.cuda(non_blocking=True)\n",
    "        if style_targets is not None:\n",
    "            style_targets = style_targets.cuda(non_blocking=True)\n",
    "\n",
    "        # Prosodic features          \n",
    "        if pitch_range is not None:\n",
    "            pitch_range = pitch_range.cuda(non_blocking=True)\n",
    "        \n",
    "        if speaking_rate is not None:\n",
    "            speaking_rate = speaking_rate.cuda(non_blocking=True)\n",
    "\n",
    "        if energy is not None:\n",
    "            energy = energy.cuda(non_blocking=True)\n",
    "\n",
    "    return text_input, text_lengths, mel_input, mel_lengths, linear_input, stop_targets, speaker_ids, \\\n",
    "        speaker_embeddings, avg_text_length, avg_spec_length, style_targets, pitch_range, speaking_rate, energy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = './experiments/debug_prosodic_features/config_direct.json'\n",
    "experiment_folder = './experiments/debug_prosodic_features/'\n",
    "c = load_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = experiment_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > num_mels:80\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:50.0\n",
      " | > mel_fmax:7600.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > stats_path:\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n"
     ]
    }
   ],
   "source": [
    "ap = AudioProcessor(**c.audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Found 10 files in D:\\Mestrado\\Emotion Audio Synthesis (TTS)\\repo_final\\pt_etts\\experiments\\debug_gst_logits\n"
     ]
    }
   ],
   "source": [
    "meta_data_train, meta_data_eval = load_meta_data(c.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'characters' in c.keys():\n",
    "    symbols, phonemes = make_symbols(**c.characters)\n",
    "\n",
    "# DISTRUBUTED\n",
    "if num_gpus > 1:\n",
    "    init_distributed(args.rank, num_gpus, args.group_id,\n",
    "                     c.distributed[\"backend\"], c.distributed[\"url\"])\n",
    "num_chars = len(phonemes) if c.use_phonemes else len(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 2 speakers: marco, rosana\n",
      " > Using model: Tacotron2\n",
      "Training with 2 speakers and 3 styles\n",
      "Use style target = True\n",
      "Use semi supervised = False\n"
     ]
    }
   ],
   "source": [
    "restore_path = None # primeiro ele Ã© none pra criar o json\n",
    "# restore_path = experiment_folder\n",
    "\n",
    "# parse speakers\n",
    "if c.use_speaker_embedding:\n",
    "    speakers = get_speakers(meta_data_train)\n",
    "    if restore_path:\n",
    "        if c.use_external_speaker_embedding_file: # if restore checkpoint and use External Embedding file\n",
    "            prev_out_path = os.path.dirname(args.restore_path)\n",
    "            speaker_mapping = load_speaker_mapping(prev_out_path)\n",
    "            if not speaker_mapping:\n",
    "                print(\"WARNING: speakers.json was not found in restore_path, trying to use CONFIG.external_speaker_embedding_file\")\n",
    "                speaker_mapping = load_speaker_mapping(c.external_speaker_embedding_file)\n",
    "                if not speaker_mapping:\n",
    "                    raise RuntimeError(\"You must copy the file speakers.json to restore_path, or set a valid file in CONFIG.external_speaker_embedding_file\")\n",
    "            speaker_embedding_dim = len(speaker_mapping[list(speaker_mapping.keys())[0]]['embedding'])\n",
    "        elif not c.use_external_speaker_embedding_file: # if restore checkpoint and don't use External Embedding file\n",
    "            prev_out_path = os.path.dirname(restore_path)\n",
    "            speaker_mapping = load_speaker_mapping(prev_out_path)\n",
    "            speaker_embedding_dim = None\n",
    "            assert all([speaker in speaker_mapping\n",
    "                        for speaker in speakers]), \"As of now you, you cannot \" \\\n",
    "                                                \"introduce new speakers to \" \\\n",
    "                                                \"a previously trained model.\"\n",
    "    else: # if start new train and don't use External Embedding file\n",
    "        speaker_mapping = {name: i for i, name in enumerate(speakers)}\n",
    "        speaker_embedding_dim = None\n",
    "    save_speaker_mapping(OUT_PATH, speaker_mapping)\n",
    "    num_speakers = len(speaker_mapping)\n",
    "    print(\"Training with {} speakers: {}\".format(num_speakers,\n",
    "                                                 \", \".join(speakers)))\n",
    "else:\n",
    "    num_speakers = 0\n",
    "    speaker_embedding_dim = None\n",
    "    speaker_mapping = None\n",
    "    \n",
    "    \n",
    "# parse styles\n",
    "if((c.use_style_embedding) | (c.use_style_lookup)):\n",
    "    styles = get_styles(meta_data_train)\n",
    "    if restore_path:\n",
    "        prev_out_path = os.path.dirname(restore_path)\n",
    "        style_mapping = load_style_mapping(prev_out_path)\n",
    "        style_embedding_dim = None\n",
    "        assert all([style in style_mapping\n",
    "                    for style in styles]), \"As of now you, you cannot \" \\\n",
    "                                            \"introduce new styles to \" \\\n",
    "                                            \"a previously trained model.\"\n",
    "    else: # if start new train and don't use External Embedding file\n",
    "        style_mapping = {name: i for i, name in enumerate(styles)}\n",
    "        style_embedding_dim = None\n",
    "    save_style_mapping(OUT_PATH, style_mapping)\n",
    "    num_styles = len(style_mapping)\n",
    "    print(\"Training with {} styles: {}\".format(num_styles,\n",
    "                                                 \", \".join(styles)))\n",
    "else:\n",
    "    num_styles = 3\n",
    "    style_embedding_dim = None\n",
    "    style_mapping = None\n",
    "\n",
    "model = setup_model(num_chars, num_speakers, num_styles, c, speaker_embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(\"./experiments/debug_style_lookup/best_model.pth.tar\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tacotron2(\n",
      "  (speaker_embedding): Embedding(2, 64)\n",
      "  (embedding): Embedding(217, 512, padding_idx=0)\n",
      "  (encoder): Encoder(\n",
      "    (convolutions): ModuleList(\n",
      "      (0): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (1): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (2): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (lstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (prenet): Prenet(\n",
      "      (linear_layers): ModuleList(\n",
      "        (0): Linear(\n",
      "          (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): Linear(\n",
      "          (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (attention_rnn): LSTMCell(835, 1024)\n",
      "    (attention): OriginalAttention(\n",
      "      (query_layer): Linear(\n",
      "        (linear_layer): Linear(in_features=1024, out_features=128, bias=False)\n",
      "      )\n",
      "      (inputs_layer): Linear(\n",
      "        (linear_layer): Linear(in_features=579, out_features=128, bias=False)\n",
      "      )\n",
      "      (v): Linear(\n",
      "        (linear_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "      )\n",
      "      (location_layer): LocationLayer(\n",
      "        (location_conv1d): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
      "        (location_dense): Linear(\n",
      "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder_rnn): LSTMCell(1603, 1024)\n",
      "    (linear_projection): Linear(\n",
      "      (linear_layer): Linear(in_features=1603, out_features=560, bias=True)\n",
      "    )\n",
      "    (stopnet): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(\n",
      "        (linear_layer): Linear(in_features=1584, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (postnet): Postnet(\n",
      "    (convolutions): ModuleList(\n",
      "      (0): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (1): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (2): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (3): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (4): ConvBNBlock(\n",
      "        (convolution1d): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (batch_normalization): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (activation): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (coarse_decoder): Decoder(\n",
      "    (prenet): Prenet(\n",
      "      (linear_layers): ModuleList(\n",
      "        (0): Linear(\n",
      "          (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): Linear(\n",
      "          (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (attention_rnn): LSTMCell(835, 1024)\n",
      "    (attention): OriginalAttention(\n",
      "      (query_layer): Linear(\n",
      "        (linear_layer): Linear(in_features=1024, out_features=128, bias=False)\n",
      "      )\n",
      "      (inputs_layer): Linear(\n",
      "        (linear_layer): Linear(in_features=579, out_features=128, bias=False)\n",
      "      )\n",
      "      (v): Linear(\n",
      "        (linear_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "      )\n",
      "      (location_layer): LocationLayer(\n",
      "        (location_conv1d): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
      "        (location_dense): Linear(\n",
      "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder_rnn): LSTMCell(1603, 1024)\n",
      "    (linear_projection): Linear(\n",
      "      (linear_layer): Linear(in_features=1603, out_features=560, bias=True)\n",
      "    )\n",
      "    (stopnet): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(\n",
      "        (linear_layer): Linear(in_features=1584, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.linear_style_target_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (prenet): Prenet(\n",
      "    (linear_layers): ModuleList(\n",
      "      (0): Linear(\n",
      "        (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n",
      "      )\n",
      "      (1): Linear(\n",
      "        (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (attention_rnn): LSTMCell(835, 1024)\n",
      "  (attention): OriginalAttention(\n",
      "    (query_layer): Linear(\n",
      "      (linear_layer): Linear(in_features=1024, out_features=128, bias=False)\n",
      "    )\n",
      "    (inputs_layer): Linear(\n",
      "      (linear_layer): Linear(in_features=579, out_features=128, bias=False)\n",
      "    )\n",
      "    (v): Linear(\n",
      "      (linear_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "    (location_layer): LocationLayer(\n",
      "      (location_conv1d): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
      "      (location_dense): Linear(\n",
      "        (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder_rnn): LSTMCell(1603, 1024)\n",
      "  (linear_projection): Linear(\n",
      "    (linear_layer): Linear(in_features=1603, out_features=560, bias=True)\n",
      "  )\n",
      "  (stopnet): Sequential(\n",
      "    (0): Dropout(p=0.1, inplace=False)\n",
      "    (1): Linear(\n",
      "      (linear_layer): Linear(in_features=1584, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " > DataLoader initialization\n",
      " | > Use phonemes: False\n",
      " | > Number of instances : 10\n",
      " | > Max length sequence: 23\n",
      " | > Min length sequence: 2\n",
      " | > Avg length sequence: 13.1\n",
      " | > Num. instances discarded by max-min (max=153, min=6) seq limits: 1\n",
      " | > Batch group size: 128.\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "data_loader = setup_loader(ap, model.decoder.r, is_val=False,\n",
    "                           verbose=(epoch == 0), speaker_mapping=speaker_mapping, style_mapping = style_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for data in data_loader:\n",
    "    text_input, text_lengths, mel_input, mel_lengths, linear_input, stop_targets, speaker_ids, speaker_embeddings, avg_text_length, avg_spec_length, style_targets, \\\n",
    "    pitch_range, speaking_rate, energy = format_data(data, speaker_mapping, style_mapping)\n",
    "    # forward pass model\n",
    "    if c.bidirectional_decoder or c.double_decoder_consistency:\n",
    "        decoder_output, postnet_output, alignments, stop_tokens, decoder_backward_output, alignments_backward, logits = model.cuda()(\n",
    "            text_input, text_lengths, mel_input, mel_lengths, speaker_ids=speaker_ids, speaker_embeddings=speaker_embeddings,\n",
    "            pitch_range = pitch_range, speaking_rate=speaking_rate, energy=energy, style_ids = style_targets)\n",
    "    else:\n",
    "        decoder_output, postnet_output, alignments, stop_tokens, logits = model.cuda()(\n",
    "            text_input, text_lengths, mel_input, mel_lengths, speaker_ids=speaker_ids, speaker_embeddings=speaker_embeddings,\n",
    "            pitch_range = pitch_range, speaking_rate=speaking_rate, energy=energy, style_ids = style_targets)\n",
    "        decoder_backward_output = None\n",
    "        alignments_backward = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((9,1))\n",
    "y = torch.rand((9,1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((x.unsqueeze(1),y), -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear_style_target_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pitch_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mask, output_mask = model.compute_masks(text_lengths, mel_lengths)\n",
    "# B x D_embed x T_in_max\n",
    "embedded_inputs = model.embedding(text_input).transpose(1, 2)\n",
    "# B x T_in_max x D_en\n",
    "encoder_outputs = model.encoder(embedded_inputs, text_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_out, gst_out, logits = model.compute_gst(encoder_outputs, mel_input, None, True, style_targets,style_targets,style_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_out.shape, encoder_outputs.shape, gst_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_targets.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._concat_speaker_embedding(enc_out, style_targets.unsqueeze(1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gst_outputs, logits = model.gst_layer(mel_input, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gst_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_range.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((gst_outputs, pitch_range.unsqueeze(1).unsqueeze(1)), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m_audio",
   "language": "python",
   "name": "m_audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
