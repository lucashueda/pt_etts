{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from matplotlib import rcParams \n",
    "rcParams[\"figure.figsize\"] = (16,5)\n",
    "sys.path.append('')\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from TTS.tts.models.tacotron2 import Tacotron2 \n",
    "from TTS.tts.utils import *\n",
    "from TTS.tts.utils.generic_utils import setup_model\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.utils.io import load_config\n",
    "from TTS.tts.utils.text import text_to_sequence, phoneme_to_sequence\n",
    "from TTS.tts.utils.text.symbols import symbols, phonemes\n",
    "import torch\n",
    "from TTS.tts.utils.synthesis import synthesis\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > num_mels:80\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:50.0\n",
      " | > mel_fmax:7600.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > stats_path:\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Using model: Tacotron2\n",
      "Training with 3 speakers and 2 styles\n",
      "Use style target = True\n",
      "Use semi supervised = False\n"
     ]
    }
   ],
   "source": [
    "# Set constants\n",
    "\n",
    "# MODEL_PATH = '/content/drive/MyDrive/Mestrado/TTS/GST_rosana_only/checkpoint_120000.pth.tar'\n",
    "CONFIG_PATH =  './experiments/debug_linear_logits/config.json'\n",
    "\n",
    "# MODEL_PATH ='/content/drive/MyDrive/Mestrado/TTS/GST_3speaker_CPQD/best_model.pth.tar'\n",
    "# CONFIG_PATH = '/content/drive/MyDrive/Mestrado/TTS/GST_3speaker_CPQD/config.json'\n",
    "\n",
    "CONFIG = load_config(CONFIG_PATH)\n",
    "# CONFIG['datasets'][0]['path'] = './'\n",
    "# CONFIG['output_path'] = './'\n",
    "# CONFIG['audio']['signal_norm'] = False\n",
    "# CONFIG['audio']['stats_path'] = ''\n",
    "# CONFIG['use_phonemes'] = False\n",
    "# CONFIG['save_step'] = 500\n",
    "use_cuda = False\n",
    "CONFIG['num_prosodic_features'] = 0\n",
    "CONFIG['agg_style_space'] = False\n",
    "CONFIG['lookup_speaker_dim'] = 512\n",
    "CONFIG['use_style_lookup'] = False\n",
    "CONFIG['lookup_style_dim'] = 64\n",
    "CONFIG['use_prosodic_linear'] = False\n",
    "CONFIG['prosodic_dim'] = 2\n",
    "# load the model\n",
    "ap = AudioProcessor(**CONFIG.audio)\n",
    "\n",
    "num_chars = len(phonemes) if CONFIG.use_phonemes else len(symbols)\n",
    "\n",
    "\n",
    "# load the model\n",
    "num_chars = len(phonemes) if CONFIG.use_phonemes else len(symbols)\n",
    "model = setup_model(num_chars, 3, 2, CONFIG, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tacotron2(\n",
       "  (speaker_embedding): Embedding(3, 512)\n",
       "  (embedding): Embedding(217, 512, padding_idx=0)\n",
       "  (encoder): Encoder(\n",
       "    (convolutions): ModuleList(\n",
       "      (0): ConvBNBlock(\n",
       "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (1): ConvBNBlock(\n",
       "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (2): ConvBNBlock(\n",
       "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (lstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (prenet): Prenet(\n",
       "      (linear_layers): ModuleList(\n",
       "        (0): Linear(\n",
       "          (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n",
       "        )\n",
       "        (1): Linear(\n",
       "          (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (attention_rnn): LSTMCell(1792, 1024)\n",
       "    (attention): OriginalAttention(\n",
       "      (query_layer): Linear(\n",
       "        (linear_layer): Linear(in_features=1024, out_features=128, bias=False)\n",
       "      )\n",
       "      (inputs_layer): Linear(\n",
       "        (linear_layer): Linear(in_features=1536, out_features=128, bias=False)\n",
       "      )\n",
       "      (v): Linear(\n",
       "        (linear_layer): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "      (location_layer): LocationLayer(\n",
       "        (location_conv1d): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
       "        (location_dense): Linear(\n",
       "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder_rnn): LSTMCell(2560, 1024)\n",
       "    (linear_projection): Linear(\n",
       "      (linear_layer): Linear(in_features=2560, out_features=560, bias=True)\n",
       "    )\n",
       "    (stopnet): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Linear(\n",
       "        (linear_layer): Linear(in_features=1584, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (postnet): Postnet(\n",
       "    (convolutions): ModuleList(\n",
       "      (0): ConvBNBlock(\n",
       "        (convolution1d): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (1): ConvBNBlock(\n",
       "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (2): ConvBNBlock(\n",
       "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (3): ConvBNBlock(\n",
       "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (4): ConvBNBlock(\n",
       "        (convolution1d): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (batch_normalization): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (activation): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (gst_layer): GST(\n",
       "    (encoder): ReferenceEncoder(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (bns): ModuleList(\n",
       "        (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (recurrence): GRU(256, 512, batch_first=True)\n",
       "    )\n",
       "  )\n",
       "  (linear_style_target_layer): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (coarse_decoder): Decoder(\n",
       "    (prenet): Prenet(\n",
       "      (linear_layers): ModuleList(\n",
       "        (0): Linear(\n",
       "          (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n",
       "        )\n",
       "        (1): Linear(\n",
       "          (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (attention_rnn): LSTMCell(1792, 1024)\n",
       "    (attention): OriginalAttention(\n",
       "      (query_layer): Linear(\n",
       "        (linear_layer): Linear(in_features=1024, out_features=128, bias=False)\n",
       "      )\n",
       "      (inputs_layer): Linear(\n",
       "        (linear_layer): Linear(in_features=1536, out_features=128, bias=False)\n",
       "      )\n",
       "      (v): Linear(\n",
       "        (linear_layer): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "      (location_layer): LocationLayer(\n",
       "        (location_conv1d): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
       "        (location_dense): Linear(\n",
       "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder_rnn): LSTMCell(2560, 1024)\n",
       "    (linear_projection): Linear(\n",
       "      (linear_layer): Linear(in_features=2560, out_features=560, bias=True)\n",
       "    )\n",
       "    (stopnet): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Linear(\n",
       "        (linear_layer): Linear(in_features=1584, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(3, 512)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.speaker_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = (1, 3)\n",
    "test = torch.rand(size).type(torch.long)\n",
    "model.speaker_embedding(test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (2, 50, 80)\n",
    "test = torch.rand(size).type(torch.float)\n",
    "style_embed, logits = model.gst_layer(test)\n",
    "# print(style_embed.shape, logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model.linear_style_target_layer(style_embed)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"TTS/bin/train_tts.py\", line 22, in <module>\n",
      "    from TTS.tts.datasets.preprocess import load_meta_data\n",
      "ModuleNotFoundError: No module named 'TTS'\n"
     ]
    }
   ],
   "source": [
    "! python TTS/bin/train_tts.py --config_path \"./experiments/debug_linear_logits/config.json\" --experiment_folder \"./experiments/debug_linear_logits/\" --continue_path \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.07925504, -0.06209112]],\n",
       "\n",
       "       [[-0.11864974, -0.04328868]]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.squeeze(0).squeeze(0).squeeze(0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = CONFIG['gst']['gst_style_tokens']\n",
    "\n",
    "feats = np.zeros((6, N))\n",
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (2,1,2) into shape (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-1c4868f21f25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (2,1,2) into shape (2)"
     ]
    }
   ],
   "source": [
    "feats[0] = logits.squeeze(0).squeeze(0).squeeze(0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def vctk_direct(root_path, meta_file):\n",
    "    meta_path = os.path.join(root_path, meta_file)\n",
    "    \n",
    "    items = []\n",
    "    \n",
    "    with open(meta_path, 'r', encoding= 'utf-8') as f:\n",
    "        for line in f:\n",
    "            cols = line.split(',')\n",
    "            if(cols[1] == 'text'): # The first element is header in my file\n",
    "                continue\n",
    "            wav_file = cols[0]\n",
    "            text = cols[1]\n",
    "            speaker_name = cols[2][:-1]  # The last char is \"\\n\" since after this line is a breakline\n",
    "            items.append([text, wav_file, speaker_name])\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './adriana_expressivo_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d609d4868b51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0madriana\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvctk_direct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'adriana_expressivo_train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-d93a6f025234>\u001b[0m in \u001b[0;36mvctk_direct\u001b[1;34m(root_path, meta_file)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './adriana_expressivo_train.csv'"
     ]
    }
   ],
   "source": [
    "adriana = vctk_direct('./', 'adriana_expressivo_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('adriana_expressivo_train.csv', encoding='latin-1', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['phoneme_cleaners']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaner = [CONFIG.text_cleaner]\n",
    "text_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = '''Um professor, para se tornar doutor, investe, pelo menos, 8 anos de sua vida ( se partir da graduação direto pro doutorado). Se fizer graduação, mestrado e doutorado, são 10 anos. Se fizer graduação, especialização, mestrado e doutorado, lá se vão 12 anos. Sim, 12 anos de universidade. Todo esse tempo labutando com ensino, pesquisa e extensão. Produzindo, apresentando e publicando trabalhos.  Participando de debates e congressos. Dando aula. Ministrando cursos. Participando de bancas. Emitindo pareceres. Orientando artigos, projetos de conclusão de cursos, TCC's e etc, etc, etc. De repente, todo esse conhecimento arduamente produzido dentro dos parâmetros científicos e acadêmicos passa a não valer  nada ante a palavra de um esquizóide qualquer que define a forma como nós, professores, devemos conduzir o trabalho para o qual nos preparamos ao longo da vida e diuturnamente - pesquisando e estudando métodos e teorias, produzindo e submetendo nossas ideias à comunidade científica - que, todos sabemos, não é muito conhecida pela generosidade. Sem comentar os diversos perrengues -  familiares, socioemocionais, financeiros e de saúde -  que enfrentamos durante e  em virtude dessa caminhada extenuante e tão mal recompensada. Ainda assim, contra tudo e todos, persistimos cantando no mal tempo, porque temos a certeza daquilo que fazemos, e assentamos nossa autoridade e dignidade no conhecimento que produzimos. \n",
    "# Antes, a violência que se abatia sobre nós, profissionais da educação, era apenas financeira e a desvalorização social era decorrente dessa desvalorização financeira: '' Você é só uma professorazinha, ganha pouco''. \n",
    "# Agora, a violência é pior: a palavra e a autoridade intelectual do professor -  que ainda mantínhamos, apesar de tudo -  são jogados por terra. \n",
    "# Somos confrontados por pessoas que não leem, não têm conhecimento de absolutamente nada e que se informam apenas por youtubers, líderes religiosos delirantes ou figuras obscuras sem nenhum (re)conhecimento científico e que são  merecidamente proscritas do meio acadêmico.  Ainda por cima, somos chamados de '' doutrinadores'', acusados e vilipendiados das piores formas possíveis sem que os difamadores se retratem ou sofram as devidas punições por suas afirmações criminosas que têm como intuito deslegitimar, mais uma vez, a profissão docente nos violentando moralmente. E, como se não bastasse, há ''professores'' que se alinham a esse  evidente projeto de destruição da educação no país. Não me admira que a profissão esteja entre as menos procuradas pelos jovens, e que esteja, infelizmente, caminhando para a extinção.'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "s = \"ambulantes vendem os ingressos para a copa do mundo de dois mil e catorze , mas só alguns dias antes do dia do sorteio .\"\n",
    "\n",
    "# s = \"e é\"\n",
    "\n",
    "seq = np.asarray(text_to_sequence(s, text_cleaner), dtype=np.int32)\n",
    "chars_var = torch.from_numpy(seq).unsqueeze(0).cuda()\n",
    "\n",
    "# style_wav = {'1': 0.3} # Parece um token interessante\n",
    "\n",
    "# f = 0.1\n",
    "# style_wav = {}\n",
    "# n_tokens = 3\n",
    "# for i in range(n_tokens):\n",
    "#     style_wav[str(i)] = f\n",
    "\n",
    "style_wav = './data/LJSpeech/LJSpeech-1.1/wavs/LJ001-0003.wav'\n",
    "\n",
    "speaker_embedding = None\n",
    "speaker_id = 2\n",
    "\n",
    "\n",
    "# style_wav = None\n",
    "\n",
    "use_cuda = True\n",
    "\n",
    "wav, alignment, decoder_output, postnet_output, stop_tokens, _, logits = synthesis(\n",
    "    model.cuda(),\n",
    "    s,\n",
    "    CONFIG,\n",
    "    use_cuda,\n",
    "    ap,\n",
    "    speaker_id=speaker_id,\n",
    "    speaker_embedding=speaker_embedding,\n",
    "    style_wav=style_wav,\n",
    "    truncated=False,\n",
    "    enable_eos_bos_chars=CONFIG.enable_eos_bos_chars, #pylint: disable=unused-argument\n",
    "    use_griffin_lim=True,\n",
    "    do_trim_silence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3413)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0][0][0].detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3413, -0.2203], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.reshape(torch.tensor([0.0,1.0]).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 120])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_var.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3413, -0.2203]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.squeeze(0).squeeze(1).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-ea9451e75f24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2525\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[1;32m-> 2526\u001b[1;33m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[0;32m   2527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "loss = functional.binary_cross_entropy(logits.squeeze(0).detach().cpu(), torch.tensor([[0.0,1.0]]))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-5c220e6230a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2466\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2468\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "loss = functional.cross_entropy(logits.squeeze(0).squeeze(0).detach().cpu(), torch.tensor([0.0,1.0]))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gst_layer.style_token_layer.style_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seqvec(text, CONFIG):\n",
    "    text_cleaner = [CONFIG.text_cleaner]\n",
    "    # text ot phonemes to sequence vector\n",
    "    if CONFIG.use_phonemes:\n",
    "        seq = np.asarray(\n",
    "            phoneme_to_sequence(text, text_cleaner, CONFIG.phoneme_language,\n",
    "                                CONFIG.enable_eos_bos_chars,\n",
    "                                tp=CONFIG.characters if 'characters' in CONFIG.keys() else None),\n",
    "            dtype=np.int32)\n",
    "    else:\n",
    "        seq = np.asarray(text_to_sequence(text, text_cleaner, tp=CONFIG.characters if 'characters' in CONFIG.keys() else None), dtype=np.int32)\n",
    "    return seq\n",
    "\n",
    "\n",
    "def numpy_to_torch(np_array, dtype, cuda=False):\n",
    "    if np_array is None:\n",
    "        return None\n",
    "    tensor = torch.as_tensor(np_array, dtype=dtype)\n",
    "    if cuda:\n",
    "        return tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "def id_to_torch(speaker_id, cuda=False):\n",
    "    if speaker_id is not None:\n",
    "        speaker_id = np.asarray(speaker_id)\n",
    "        speaker_id = torch.from_numpy(speaker_id).unsqueeze(0)\n",
    "    if cuda:\n",
    "        return speaker_id.cuda().type(torch.long)\n",
    "    return speaker_id.type(torch.long)\n",
    "\n",
    "def compute_style_mel(style_wav, ap, cuda=False):\n",
    "    style_mel = torch.FloatTensor(ap.melspectrogram(\n",
    "        ap.load_wav(style_wav, sr=ap.sample_rate))).unsqueeze(0)\n",
    "    if cuda:\n",
    "        return style_mel.cuda()\n",
    "    return style_mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id = id_to_torch(2, cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = text_to_seqvec(s, CONFIG)\n",
    "inputs = numpy_to_torch(inputs, torch.long, cuda=use_cuda)\n",
    "inputs = inputs.unsqueeze(0)\n",
    "\n",
    "x = model.embedding(inputs).transpose(1, 2)\n",
    "x = model.encoder.inference(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'speaker_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-28d2aec601d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspeaker_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'speaker_ids' is not defined"
     ]
    }
   ],
   "source": [
    "speaker_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered None\n"
     ]
    }
   ],
   "source": [
    "speaker_ids = speaker_id\n",
    "gst = True\n",
    "num_speakers = 3\n",
    "style_mel = None\n",
    "\n",
    "if gst:\n",
    "    # B x gst_dim\n",
    "    inputs, encoder_outputs, logits = model.compute_gst(x,\n",
    "                                       style_mel,\n",
    "                                       speaker_embeddings if model.gst_use_speaker_embedding else None)\n",
    "# if num_speakers > 1:\n",
    "#     if not model.embeddings_per_sample:\n",
    "#         speaker_embeddings = model.speaker_embedding(speaker_ids)[:, None]\n",
    "#     encoder_outputs = model._concat_speaker_embedding(encoder_outputs, speaker_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0044,  0.1618, -0.0081,  ..., -0.0787, -0.0941, -0.2031],\n",
       "          [-0.1884,  0.2452, -0.0352,  ..., -0.0420,  0.0776,  0.1251],\n",
       "          [-0.1088,  0.2194,  0.1914,  ...,  0.0060,  0.1189, -0.3682],\n",
       "          ...,\n",
       "          [ 0.0192, -0.2656, -0.0227,  ..., -0.1376,  0.1479, -0.2288],\n",
       "          [-0.1095,  0.0519,  0.1441,  ..., -0.2510, -0.2060, -0.5496],\n",
       "          [-0.2093,  0.1644, -0.1617,  ..., -0.0055,  0.1545, -0.1248]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[-2.1055,  0.1618,  2.8885,  ..., -1.8753, -0.0941,  0.7225],\n",
       "          [-0.0258, -0.8962, -0.0352,  ...,  2.0481, -1.8784,  0.1251],\n",
       "          [ 0.9887,  0.2194, -0.3214,  ..., -1.2698, -1.5533, -1.6216],\n",
       "          ...,\n",
       "          [-0.2023, -0.2656, -2.2603,  ..., -0.1376,  1.3449, -0.2288],\n",
       "          [ 0.9286,  0.0519,  0.2769,  ...,  4.0197, -0.2060, -1.3710],\n",
       "          [-0.2093,  0.1644,  0.2564,  ...,  1.0668,  1.0137,  1.0870]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[0.0091, 0.0081, 0.0081, 0.0089, 0.0078, 0.0094, 0.0075, 0.0091,\n",
       "           0.0093, 0.0088, 0.0085, 0.0065, 0.0071, 0.0069, 0.0094, 0.0091,\n",
       "           0.0077, 0.0079, 0.0098, 0.0069, 0.0082, 0.0077, 0.0086, 0.0094,\n",
       "           0.0085, 0.0104, 0.0075, 0.0073, 0.0066, 0.0076, 0.0076, 0.0078,\n",
       "           0.0076, 0.0063, 0.0090, 0.0078, 0.0080, 0.0072, 0.0083, 0.0080,\n",
       "           0.0089, 0.0091, 0.0111, 0.0107, 0.0087, 0.0074, 0.0088, 0.0084,\n",
       "           0.0068, 0.0087, 0.0081, 0.0079, 0.0081, 0.0087, 0.0078, 0.0071,\n",
       "           0.0071, 0.0076, 0.0076, 0.0102, 0.0104, 0.0081, 0.0092, 0.0089,\n",
       "           0.0081, 0.0067, 0.0078, 0.0062, 0.0067, 0.0074, 0.0090, 0.0084,\n",
       "           0.0063, 0.0090, 0.0089, 0.0086, 0.0099, 0.0083, 0.0074, 0.0095,\n",
       "           0.0082, 0.0075, 0.0095, 0.0091, 0.0078, 0.0082, 0.0066, 0.0069,\n",
       "           0.0079, 0.0102, 0.0078, 0.0080, 0.0089, 0.0104, 0.0090, 0.0103,\n",
       "           0.0104, 0.0078, 0.0107, 0.0131, 0.0092, 0.0085, 0.0062, 0.0057,\n",
       "           0.0062, 0.0086, 0.0077, 0.0099, 0.0078, 0.0089, 0.0103, 0.0090,\n",
       "           0.0086, 0.0067, 0.0078, 0.0082, 0.0087, 0.0076, 0.0090, 0.0083],\n",
       "          [0.0091, 0.0082, 0.0083, 0.0090, 0.0078, 0.0095, 0.0074, 0.0090,\n",
       "           0.0092, 0.0089, 0.0085, 0.0064, 0.0070, 0.0069, 0.0094, 0.0092,\n",
       "           0.0077, 0.0080, 0.0099, 0.0068, 0.0081, 0.0077, 0.0085, 0.0093,\n",
       "           0.0084, 0.0105, 0.0074, 0.0072, 0.0065, 0.0077, 0.0075, 0.0077,\n",
       "           0.0076, 0.0063, 0.0091, 0.0077, 0.0081, 0.0071, 0.0082, 0.0080,\n",
       "           0.0089, 0.0091, 0.0111, 0.0107, 0.0086, 0.0073, 0.0088, 0.0084,\n",
       "           0.0068, 0.0088, 0.0082, 0.0079, 0.0081, 0.0087, 0.0077, 0.0070,\n",
       "           0.0071, 0.0076, 0.0076, 0.0102, 0.0104, 0.0081, 0.0092, 0.0089,\n",
       "           0.0080, 0.0067, 0.0077, 0.0062, 0.0067, 0.0074, 0.0090, 0.0083,\n",
       "           0.0063, 0.0090, 0.0089, 0.0085, 0.0099, 0.0083, 0.0074, 0.0095,\n",
       "           0.0081, 0.0074, 0.0094, 0.0091, 0.0078, 0.0083, 0.0065, 0.0070,\n",
       "           0.0081, 0.0103, 0.0080, 0.0082, 0.0089, 0.0105, 0.0090, 0.0104,\n",
       "           0.0105, 0.0078, 0.0108, 0.0131, 0.0092, 0.0084, 0.0063, 0.0058,\n",
       "           0.0062, 0.0088, 0.0078, 0.0100, 0.0078, 0.0090, 0.0103, 0.0091,\n",
       "           0.0085, 0.0066, 0.0077, 0.0082, 0.0087, 0.0076, 0.0089, 0.0083]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[0.5715],\n",
       "          [0.5804]]], device='cuda:0'),\n",
       " tensor([[[-0.0388,  0.0171]]], device='cuda:0'))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = text_to_seqvec(s, CONFIG)\n",
    "inputs = numpy_to_torch(inputs, torch.long, cuda=use_cuda)\n",
    "inputs = inputs.unsqueeze(0)\n",
    "\n",
    "model.inference(text = inputs, speaker_ids = speaker_id, style_mel = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = chars_var.device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_inputs = model.embedding(chars_var.type(torch.long)).transpose(1, 2)\n",
    "encoder_outputs = model.encoder.inference(embedded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 120, 512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'style_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-cd263e5e74b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgst_embedding_dim\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0m_GST\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgst_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle_token_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mgst_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgst_embedding_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_amplifier\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstyle_wav\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_amplifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'style_tokens'"
     ]
    }
   ],
   "source": [
    "query = torch.zeros(1, 1, model.gst_embedding_dim//2).to(device)\n",
    "_GST = torch.tanh(model.gst_layer.style_token_layer.style_tokens)\n",
    "gst_outputs = torch.zeros(1, 1, model.gst_embedding_dim).to(device)\n",
    "for k_token, v_amplifier in style_wav.items():\n",
    "    print(k_token, v_amplifier)\n",
    "    key = _GST[int(k_token)].unsqueeze(0).expand(1, -1, -1)\n",
    "    print(query.shape, key.shape)\n",
    "    gst_outputs_att, _ = model.gst_layer.style_token_layer.attention(query, key)\n",
    "    print(gst_outputs_att.shape)\n",
    "    gst_outputs = gst_outputs + gst_outputs_att * v_amplifier\n",
    "    \n",
    "gst_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gst_outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-26bc0bd34f26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencoder_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concat_speaker_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgst_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gst_outputs' is not defined"
     ]
    }
   ],
   "source": [
    "encoder_outputs = model._concat_speaker_embedding(encoder_outputs, gst_outputs)\n",
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gst_outputs_att.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gst_outputs_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_token, v_amplifier in style_wav.items():\n",
    "    print(type(v_amplifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gst_layer.style_token_layer.style_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gst_layer.style_token_layer.style_tokens[0,:]*0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_mel = compute_style_mel(style_wav, ap, cuda=True)\n",
    "style_mel = numpy_to_torch(style_mel, torch.float, cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"ambulantes vendem os ingressos para a copa do mundo de dois mil e catorze , mas só alguns dias antes do dia do sorteio .\"\n",
    "\n",
    "# inputs = text_to_seqvec(s, CONFIG)\n",
    "# inputs = numpy_to_torch(inputs, torch.long, cuda=use_cuda)\n",
    "# inputs = inputs.unsqueeze(0)\n",
    "\n",
    "# embedded_inputs = model.embedding(inputs).transpose(1, 2)\n",
    "# encoder_outputs = model.encoder.inference(embedded_inputs)\n",
    "\n",
    "# print(inputs.shape, encoder_outputs.shape )\n",
    "\n",
    "encoder, logits = model.cuda().gst_layer(style_mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.cat(\n",
    "            torch.split(logits, 1, dim=0),\n",
    "            dim=3).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m_audio",
   "language": "python",
   "name": "m_audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
