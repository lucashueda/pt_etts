{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from matplotlib import rcParams \n",
    "rcParams[\"figure.figsize\"] = (16,5)\n",
    "sys.path.append('')\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from TTS.tts.models.tacotron2 import Tacotron2 \n",
    "from TTS.tts.utils import *\n",
    "from TTS.tts.utils.generic_utils import setup_model\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.utils.io import load_config\n",
    "from TTS.tts.utils.text import text_to_sequence, phoneme_to_sequence\n",
    "from TTS.tts.utils.text.symbols import symbols, phonemes\n",
    "import torch\n",
    "from TTS.tts.utils.synthesis import synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > num_mels:80\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:512\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > stats_path:\n",
      " | > hop_length:128\n",
      " | > win_length:512\n",
      " > Using model: Tacotron2\n"
     ]
    }
   ],
   "source": [
    "# Set constants\n",
    "\n",
    "# MODEL_PATH = '/content/drive/MyDrive/Mestrado/TTS/GST_rosana_only/checkpoint_120000.pth.tar'\n",
    "CONFIG_PATH =  './experiments/debug/config.json'\n",
    "\n",
    "# MODEL_PATH ='/content/drive/MyDrive/Mestrado/TTS/GST_3speaker_CPQD/best_model.pth.tar'\n",
    "# CONFIG_PATH = '/content/drive/MyDrive/Mestrado/TTS/GST_3speaker_CPQD/config.json'\n",
    "\n",
    "CONFIG = load_config(CONFIG_PATH)\n",
    "# CONFIG['datasets'][0]['path'] = './'\n",
    "# CONFIG['output_path'] = './'\n",
    "# CONFIG['audio']['signal_norm'] = False\n",
    "# CONFIG['audio']['stats_path'] = ''\n",
    "# CONFIG['use_phonemes'] = False\n",
    "# CONFIG['save_step'] = 500\n",
    "use_cuda = True\n",
    "\n",
    "# load the model\n",
    "ap = AudioProcessor(**CONFIG.audio)\n",
    "\n",
    "num_chars = len(phonemes) if CONFIG.use_phonemes else len(symbols)\n",
    "\n",
    "\n",
    "# load the model\n",
    "num_chars = len(phonemes) if CONFIG.use_phonemes else len(symbols)\n",
    "model = setup_model(num_chars, 3, CONFIG, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tacotron2(\n",
       "  (speaker_embedding): Embedding(3, 512)\n",
       "  (embedding): Embedding(217, 512, padding_idx=0)\n",
       "  (encoder): Encoder(\n",
       "    (convolutions): ModuleList(\n",
       "      (0): ConvBNBlock(\n",
       "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (1): ConvBNBlock(\n",
       "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (2): ConvBNBlock(\n",
       "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (lstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (prenet): Prenet(\n",
       "      (linear_layers): ModuleList(\n",
       "        (0): Linear(\n",
       "          (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n",
       "        )\n",
       "        (1): Linear(\n",
       "          (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (attention_rnn): LSTMCell(1536, 1024)\n",
       "    (attention): OriginalAttention(\n",
       "      (query_layer): Linear(\n",
       "        (linear_layer): Linear(in_features=1024, out_features=128, bias=False)\n",
       "      )\n",
       "      (inputs_layer): Linear(\n",
       "        (linear_layer): Linear(in_features=1280, out_features=128, bias=False)\n",
       "      )\n",
       "      (v): Linear(\n",
       "        (linear_layer): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "      (location_layer): LocationLayer(\n",
       "        (location_conv1d): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
       "        (location_dense): Linear(\n",
       "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder_rnn): LSTMCell(2304, 1024)\n",
       "    (linear_projection): Linear(\n",
       "      (linear_layer): Linear(in_features=2304, out_features=560, bias=True)\n",
       "    )\n",
       "    (stopnet): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Linear(\n",
       "        (linear_layer): Linear(in_features=1584, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (postnet): Postnet(\n",
       "    (convolutions): ModuleList(\n",
       "      (0): ConvBNBlock(\n",
       "        (convolution1d): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (1): ConvBNBlock(\n",
       "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (2): ConvBNBlock(\n",
       "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (3): ConvBNBlock(\n",
       "        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (4): ConvBNBlock(\n",
       "        (convolution1d): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (batch_normalization): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (activation): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (gst_layer): GST(\n",
       "    (encoder): ReferenceEncoder(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (bns): ModuleList(\n",
       "        (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (recurrence): GRU(256, 128, batch_first=True)\n",
       "    )\n",
       "    (style_token_layer): StyleTokenLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=128, out_features=256, bias=False)\n",
       "        (W_key): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (W_value): Linear(in_features=256, out_features=256, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (coarse_decoder): Decoder(\n",
       "    (prenet): Prenet(\n",
       "      (linear_layers): ModuleList(\n",
       "        (0): Linear(\n",
       "          (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n",
       "        )\n",
       "        (1): Linear(\n",
       "          (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (attention_rnn): LSTMCell(1536, 1024)\n",
       "    (attention): OriginalAttention(\n",
       "      (query_layer): Linear(\n",
       "        (linear_layer): Linear(in_features=1024, out_features=128, bias=False)\n",
       "      )\n",
       "      (inputs_layer): Linear(\n",
       "        (linear_layer): Linear(in_features=1280, out_features=128, bias=False)\n",
       "      )\n",
       "      (v): Linear(\n",
       "        (linear_layer): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "      (location_layer): LocationLayer(\n",
       "        (location_conv1d): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
       "        (location_dense): Linear(\n",
       "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder_rnn): LSTMCell(2304, 1024)\n",
       "    (linear_projection): Linear(\n",
       "      (linear_layer): Linear(in_features=2304, out_features=400, bias=True)\n",
       "    )\n",
       "    (stopnet): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Linear(\n",
       "        (linear_layer): Linear(in_features=1424, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(3, 512)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.speaker_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = (1, 3)\n",
    "test = torch.rand(size).type(torch.long)\n",
    "model.speaker_embedding(test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256]) torch.Size([1, 1, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "size = (1, 50, 80)\n",
    "test = torch.rand(size).type(torch.float)\n",
    "style_embed, logits = model.gst_layer(test)\n",
    "print(style_embed.shape, logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33001867, 0.3326895 , 0.33729187], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.squeeze(0).squeeze(0).squeeze(0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = CONFIG['gst']['gst_style_tokens']\n",
    "\n",
    "feats = np.zeros((6, N))\n",
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats[0] = logits.squeeze(0).squeeze(0).squeeze(0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33001867, 0.33268949, 0.33729187],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def vctk_direct(root_path, meta_file):\n",
    "    meta_path = os.path.join(root_path, meta_file)\n",
    "    \n",
    "    items = []\n",
    "    \n",
    "    with open(meta_path, 'r', encoding= 'utf-8') as f:\n",
    "        for line in f:\n",
    "            cols = line.split(',')\n",
    "            if(cols[1] == 'text'): # The first element is header in my file\n",
    "                continue\n",
    "            wav_file = cols[0]\n",
    "            text = cols[1]\n",
    "            speaker_name = cols[2][:-1]  # The last char is \"\\n\" since after this line is a breakline\n",
    "            items.append([text, wav_file, speaker_name])\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode bytes in position 296-297: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-d609d4868b51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0madriana\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvctk_direct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'adriana_expressivo_train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-59-d93a6f025234>\u001b[0m in \u001b[0;36mvctk_direct\u001b[1;34m(root_path, meta_file)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# The first element is header in my file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\m_audio\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[1;31m# keep undecoded input until the next call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 296-297: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "adriana = vctk_direct('./', 'adriana_expressivo_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('adriana_expressivo_train.csv', encoding='latin-1', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wav_path</th>\n",
       "      <th>text</th>\n",
       "      <th>emb_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/l/disk1/awstebas/data/TTS/vozes_sync/Adriana/...</td>\n",
       "      <td>os leitores podem colaborar com esta rodada do...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/l/disk1/awstebas/data/TTS/vozes_sync/Adriana/...</td>\n",
       "      <td>e a notãâ­cia boa ãâ© que o raio x mostra qu...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/l/disk1/awstebas/data/TTS/vozes_sync/Adriana/...</td>\n",
       "      <td>ãâ impossãâ­vel nãâ£o gostar da histãâ³ri...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/l/disk1/awstebas/data/TTS/vozes_sync/Adriana/...</td>\n",
       "      <td>se for ao reino da sibãâ©ria , busca ajuda da...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/l/disk1/awstebas/data/TTS/vozes_sync/Adriana/...</td>\n",
       "      <td>jãâºnior , que investiga a guerra civil cario...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            wav_path  \\\n",
       "0  /l/disk1/awstebas/data/TTS/vozes_sync/Adriana/...   \n",
       "1  /l/disk1/awstebas/data/TTS/vozes_sync/Adriana/...   \n",
       "2  /l/disk1/awstebas/data/TTS/vozes_sync/Adriana/...   \n",
       "3  /l/disk1/awstebas/data/TTS/vozes_sync/Adriana/...   \n",
       "4  /l/disk1/awstebas/data/TTS/vozes_sync/Adriana/...   \n",
       "\n",
       "                                                text  emb_id  \n",
       "0  os leitores podem colaborar com esta rodada do...       3  \n",
       "1  e a notãâ­cia boa ãâ© que o raio x mostra qu...       3  \n",
       "2  ãâ impossãâ­vel nãâ£o gostar da histãâ³ri...       3  \n",
       "3  se for ao reino da sibãâ©ria , busca ajuda da...       3  \n",
       "4  jãâºnior , que investiga a guerra civil cario...       3  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.3378, 0.3444, 0.3178]],\n",
       "\n",
       "         [[0.3369, 0.3411, 0.3220]],\n",
       "\n",
       "         [[0.3310, 0.3294, 0.3396]],\n",
       "\n",
       "         [[0.3223, 0.3372, 0.3405]]]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['basic_cleaners']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaner = [CONFIG.text_cleaner]\n",
    "text_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = '''Um professor, para se tornar doutor, investe, pelo menos, 8 anos de sua vida ( se partir da graduação direto pro doutorado). Se fizer graduação, mestrado e doutorado, são 10 anos. Se fizer graduação, especialização, mestrado e doutorado, lá se vão 12 anos. Sim, 12 anos de universidade. Todo esse tempo labutando com ensino, pesquisa e extensão. Produzindo, apresentando e publicando trabalhos.  Participando de debates e congressos. Dando aula. Ministrando cursos. Participando de bancas. Emitindo pareceres. Orientando artigos, projetos de conclusão de cursos, TCC's e etc, etc, etc. De repente, todo esse conhecimento arduamente produzido dentro dos parâmetros científicos e acadêmicos passa a não valer  nada ante a palavra de um esquizóide qualquer que define a forma como nós, professores, devemos conduzir o trabalho para o qual nos preparamos ao longo da vida e diuturnamente - pesquisando e estudando métodos e teorias, produzindo e submetendo nossas ideias à comunidade científica - que, todos sabemos, não é muito conhecida pela generosidade. Sem comentar os diversos perrengues -  familiares, socioemocionais, financeiros e de saúde -  que enfrentamos durante e  em virtude dessa caminhada extenuante e tão mal recompensada. Ainda assim, contra tudo e todos, persistimos cantando no mal tempo, porque temos a certeza daquilo que fazemos, e assentamos nossa autoridade e dignidade no conhecimento que produzimos. \n",
    "# Antes, a violência que se abatia sobre nós, profissionais da educação, era apenas financeira e a desvalorização social era decorrente dessa desvalorização financeira: '' Você é só uma professorazinha, ganha pouco''. \n",
    "# Agora, a violência é pior: a palavra e a autoridade intelectual do professor -  que ainda mantínhamos, apesar de tudo -  são jogados por terra. \n",
    "# Somos confrontados por pessoas que não leem, não têm conhecimento de absolutamente nada e que se informam apenas por youtubers, líderes religiosos delirantes ou figuras obscuras sem nenhum (re)conhecimento científico e que são  merecidamente proscritas do meio acadêmico.  Ainda por cima, somos chamados de '' doutrinadores'', acusados e vilipendiados das piores formas possíveis sem que os difamadores se retratem ou sofram as devidas punições por suas afirmações criminosas que têm como intuito deslegitimar, mais uma vez, a profissão docente nos violentando moralmente. E, como se não bastasse, há ''professores'' que se alinham a esse  evidente projeto de destruição da educação no país. Não me admira que a profissão esteja entre as menos procuradas pelos jovens, e que esteja, infelizmente, caminhando para a extinção.'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "s = \"ambulantes vendem os ingressos para a copa do mundo de dois mil e catorze , mas só alguns dias antes do dia do sorteio .\"\n",
    "\n",
    "# s = \"e é\"\n",
    "\n",
    "seq = np.asarray(text_to_sequence(s, text_cleaner), dtype=np.int32)\n",
    "chars_var = torch.from_numpy(seq).unsqueeze(0).cuda()\n",
    "\n",
    "# style_wav = {'1': 0.3} # Parece um token interessante\n",
    "\n",
    "# f = 0.1\n",
    "# style_wav = {}\n",
    "# n_tokens = 3\n",
    "# for i in range(n_tokens):\n",
    "#     style_wav[str(i)] = f\n",
    "\n",
    "style_wav = './data/LJSpeech/LJSpeech-1.1/wavs/LJ001-0003.wav'\n",
    "\n",
    "speaker_embedding = None\n",
    "speaker_id = 2\n",
    "\n",
    "\n",
    "# style_wav = None\n",
    "\n",
    "use_cuda = True\n",
    "\n",
    "wav, alignment, decoder_output, postnet_output, stop_tokens, _, logits = synthesis(\n",
    "    model.cuda(),\n",
    "    s,\n",
    "    CONFIG,\n",
    "    use_cuda,\n",
    "    ap,\n",
    "    speaker_id=speaker_id,\n",
    "    speaker_embedding=speaker_embedding,\n",
    "    style_wav=style_wav,\n",
    "    truncated=False,\n",
    "    enable_eos_bos_chars=CONFIG.enable_eos_bos_chars, #pylint: disable=unused-argument\n",
    "    use_griffin_lim=True,\n",
    "    do_trim_silence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2940, 0.3483, 0.3577])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0][0][0].detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2940, 0.3483, 0.3577], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.reshape(torch.tensor([0.0,0.0,1.0]).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 120])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_var.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0.0,0.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([1, 1, 1, 3])) is deprecated. Please ensure they have the same size.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6158)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = functional.binary_cross_entropy(logits.detach().cpu(), torch.tensor([0.0,0.0,1.0]))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 120)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/LJSpeech/LJSpeech-1.1/wavs/LJ001-0003.wav'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gst_layer.style_token_layer.style_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seqvec(text, CONFIG):\n",
    "    text_cleaner = [CONFIG.text_cleaner]\n",
    "    # text ot phonemes to sequence vector\n",
    "    if CONFIG.use_phonemes:\n",
    "        seq = np.asarray(\n",
    "            phoneme_to_sequence(text, text_cleaner, CONFIG.phoneme_language,\n",
    "                                CONFIG.enable_eos_bos_chars,\n",
    "                                tp=CONFIG.characters if 'characters' in CONFIG.keys() else None),\n",
    "            dtype=np.int32)\n",
    "    else:\n",
    "        seq = np.asarray(text_to_sequence(text, text_cleaner, tp=CONFIG.characters if 'characters' in CONFIG.keys() else None), dtype=np.int32)\n",
    "    return seq\n",
    "\n",
    "\n",
    "def numpy_to_torch(np_array, dtype, cuda=False):\n",
    "    if np_array is None:\n",
    "        return None\n",
    "    tensor = torch.as_tensor(np_array, dtype=dtype)\n",
    "    if cuda:\n",
    "        return tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "def id_to_torch(speaker_id, cuda=False):\n",
    "    if speaker_id is not None:\n",
    "        speaker_id = np.asarray(speaker_id)\n",
    "        speaker_id = torch.from_numpy(speaker_id).unsqueeze(0)\n",
    "    if cuda:\n",
    "        return speaker_id.cuda().type(torch.long)\n",
    "    return speaker_id.type(torch.long)\n",
    "\n",
    "def compute_style_mel(style_wav, ap, cuda=False):\n",
    "    style_mel = torch.FloatTensor(ap.melspectrogram(\n",
    "        ap.load_wav(style_wav, sr=ap.sample_rate))).unsqueeze(0)\n",
    "    if cuda:\n",
    "        return style_mel.cuda()\n",
    "    return style_mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id = id_to_torch(2, cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = text_to_seqvec(s, CONFIG)\n",
    "inputs = numpy_to_torch(inputs, torch.long, cuda=use_cuda)\n",
    "inputs = inputs.unsqueeze(0)\n",
    "\n",
    "x = model.embedding(inputs).transpose(1, 2)\n",
    "x = model.encoder.inference(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_ids = speaker_id\n",
    "gst = True\n",
    "num_speakers = 3\n",
    "style_mel = None\n",
    "\n",
    "if gst:\n",
    "    # B x gst_dim\n",
    "    encoder_outputs, logits = model.compute_gst(x,\n",
    "                                       style_mel,\n",
    "                                       speaker_embeddings if model.gst_use_speaker_embedding else None)\n",
    "if num_speakers > 1:\n",
    "    if not model.embeddings_per_sample:\n",
    "        speaker_embeddings = model.speaker_embedding(speaker_ids)[:, None]\n",
    "    encoder_outputs = model._concat_speaker_embedding(encoder_outputs, speaker_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0305, -0.0450, -0.0005,  ...,  0.1477, -0.0312,  0.1719],\n",
       "          [ 0.1300,  0.0743, -0.0043,  ...,  0.1124, -0.1079, -0.2240],\n",
       "          [-0.1467,  0.3013,  0.0639,  ...,  0.0745, -0.0475, -0.0513],\n",
       "          ...,\n",
       "          [-0.2115,  0.2265, -0.1254,  ...,  0.0058,  0.1708,  0.0503],\n",
       "          [ 0.0314, -0.0464,  0.0146,  ..., -0.0792,  0.3024, -0.1121],\n",
       "          [ 0.0527, -0.1534, -0.0071,  ..., -0.0079, -0.2290,  0.1016]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[-3.0516e-02,  8.6851e-01, -4.7641e-04,  ...,  1.4774e-01,\n",
       "           -3.1177e-02,  1.7193e-01],\n",
       "          [ 1.2995e-01,  7.4270e-02, -4.2638e-03,  ...,  1.1239e-01,\n",
       "           -3.6288e+00, -2.2402e-01],\n",
       "          [-4.6684e-01,  3.0128e-01,  6.3911e-02,  ..., -2.0052e+00,\n",
       "            1.1703e+00,  1.1048e+00],\n",
       "          ...,\n",
       "          [-2.1146e-01,  2.2655e-01,  1.7077e+00,  ...,  5.7870e-03,\n",
       "            1.7076e-01,  5.0268e-02],\n",
       "          [ 3.1421e-02, -1.0175e-01,  1.4641e-02,  ...,  6.6859e-01,\n",
       "            3.0241e-01,  2.0838e+00],\n",
       "          [ 6.7483e-01, -2.1270e+00, -7.1373e-03,  ..., -7.8935e-03,\n",
       "            1.9161e-01,  1.0164e-01]]], device='cuda:0'),\n",
       " tensor([[[0.0066, 0.0085, 0.0106, 0.0083, 0.0076, 0.0069, 0.0072, 0.0092,\n",
       "           0.0067, 0.0058, 0.0059, 0.0070, 0.0061, 0.0083, 0.0097, 0.0100,\n",
       "           0.0086, 0.0102, 0.0084, 0.0081, 0.0075, 0.0089, 0.0091, 0.0106,\n",
       "           0.0098, 0.0078, 0.0078, 0.0087, 0.0087, 0.0066, 0.0092, 0.0089,\n",
       "           0.0116, 0.0099, 0.0104, 0.0092, 0.0096, 0.0081, 0.0083, 0.0091,\n",
       "           0.0095, 0.0094, 0.0070, 0.0082, 0.0083, 0.0084, 0.0080, 0.0071,\n",
       "           0.0104, 0.0115, 0.0103, 0.0087, 0.0094, 0.0062, 0.0069, 0.0070,\n",
       "           0.0071, 0.0080, 0.0062, 0.0080, 0.0095, 0.0088, 0.0085, 0.0076,\n",
       "           0.0080, 0.0083, 0.0081, 0.0058, 0.0068, 0.0079, 0.0073, 0.0097,\n",
       "           0.0099, 0.0088, 0.0096, 0.0075, 0.0102, 0.0077, 0.0070, 0.0085,\n",
       "           0.0080, 0.0079, 0.0089, 0.0080, 0.0091, 0.0081, 0.0088, 0.0115,\n",
       "           0.0110, 0.0090, 0.0073, 0.0066, 0.0075, 0.0074, 0.0092, 0.0107,\n",
       "           0.0071, 0.0085, 0.0081, 0.0119, 0.0074, 0.0073, 0.0064, 0.0075,\n",
       "           0.0062, 0.0072, 0.0063, 0.0068, 0.0075, 0.0059, 0.0073, 0.0078,\n",
       "           0.0070, 0.0106, 0.0116, 0.0090, 0.0070, 0.0088, 0.0081, 0.0090],\n",
       "          [0.0066, 0.0085, 0.0107, 0.0083, 0.0075, 0.0068, 0.0072, 0.0093,\n",
       "           0.0068, 0.0058, 0.0058, 0.0070, 0.0061, 0.0084, 0.0097, 0.0100,\n",
       "           0.0087, 0.0103, 0.0084, 0.0081, 0.0076, 0.0090, 0.0091, 0.0106,\n",
       "           0.0098, 0.0077, 0.0077, 0.0085, 0.0087, 0.0066, 0.0091, 0.0089,\n",
       "           0.0117, 0.0099, 0.0104, 0.0091, 0.0096, 0.0082, 0.0084, 0.0092,\n",
       "           0.0095, 0.0094, 0.0070, 0.0082, 0.0084, 0.0083, 0.0080, 0.0071,\n",
       "           0.0104, 0.0114, 0.0103, 0.0087, 0.0094, 0.0062, 0.0069, 0.0071,\n",
       "           0.0071, 0.0080, 0.0062, 0.0079, 0.0094, 0.0088, 0.0086, 0.0077,\n",
       "           0.0081, 0.0083, 0.0081, 0.0058, 0.0068, 0.0079, 0.0073, 0.0097,\n",
       "           0.0100, 0.0089, 0.0096, 0.0075, 0.0101, 0.0077, 0.0071, 0.0085,\n",
       "           0.0079, 0.0079, 0.0090, 0.0080, 0.0092, 0.0081, 0.0088, 0.0115,\n",
       "           0.0109, 0.0090, 0.0073, 0.0066, 0.0075, 0.0074, 0.0092, 0.0108,\n",
       "           0.0072, 0.0084, 0.0080, 0.0118, 0.0075, 0.0073, 0.0064, 0.0075,\n",
       "           0.0062, 0.0072, 0.0064, 0.0068, 0.0075, 0.0060, 0.0074, 0.0077,\n",
       "           0.0069, 0.0105, 0.0116, 0.0090, 0.0069, 0.0088, 0.0081, 0.0090]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[0.5196],\n",
       "          [0.5063]]], device='cuda:0'),\n",
       " None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = text_to_seqvec(s, CONFIG)\n",
    "inputs = numpy_to_torch(inputs, torch.long, cuda=use_cuda)\n",
    "inputs = inputs.unsqueeze(0)\n",
    "\n",
    "model.inference(text = inputs, speaker_ids = speaker_id, style_mel = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = chars_var.device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_inputs = model.embedding(chars_var.type(torch.long)).transpose(1, 2)\n",
    "encoder_outputs = model.encoder.inference(embedded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 120, 512])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-cd263e5e74b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0m_GST\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgst_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle_token_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgst_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgst_embedding_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mk_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_amplifier\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstyle_wav\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_amplifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_GST\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "query = torch.zeros(1, 1, model.gst_embedding_dim//2).to(device)\n",
    "_GST = torch.tanh(model.gst_layer.style_token_layer.style_tokens)\n",
    "gst_outputs = torch.zeros(1, 1, model.gst_embedding_dim).to(device)\n",
    "for k_token, v_amplifier in style_wav.items():\n",
    "    print(k_token, v_amplifier)\n",
    "    key = _GST[int(k_token)].unsqueeze(0).expand(1, -1, -1)\n",
    "    print(query.shape, key.shape)\n",
    "    gst_outputs_att, _ = model.gst_layer.style_token_layer.attention(query, key)\n",
    "    print(gst_outputs_att.shape)\n",
    "    gst_outputs = gst_outputs + gst_outputs_att * v_amplifier\n",
    "    \n",
    "gst_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = model._concat_speaker_embedding(encoder_outputs, gst_outputs)\n",
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gst_outputs_att.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gst_outputs_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_token, v_amplifier in style_wav.items():\n",
    "    print(type(v_amplifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gst_layer.style_token_layer.style_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gst_layer.style_token_layer.style_tokens[0,:]*0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_mel = compute_style_mel(style_wav, ap, cuda=True)\n",
    "style_mel = numpy_to_torch(style_mel, torch.float, cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 1206])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"ambulantes vendem os ingressos para a copa do mundo de dois mil e catorze , mas só alguns dias antes do dia do sorteio .\"\n",
    "\n",
    "# inputs = text_to_seqvec(s, CONFIG)\n",
    "# inputs = numpy_to_torch(inputs, torch.long, cuda=use_cuda)\n",
    "# inputs = inputs.unsqueeze(0)\n",
    "\n",
    "# embedded_inputs = model.embedding(inputs).transpose(1, 2)\n",
    "# encoder_outputs = model.encoder.inference(embedded_inputs)\n",
    "\n",
    "# print(inputs.shape, encoder_outputs.shape )\n",
    "\n",
    "encoder, logits = model.cuda().gst_layer(style_mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.3521, 0.3196, 0.3283]]]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.cat(\n",
    "            torch.split(logits, 1, dim=0),\n",
    "            dim=3).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1, 3])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m_audio",
   "language": "python",
   "name": "m_audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
