{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.insert(0, 'D:\\\\Mestrado\\\\Emotion Audio Synthesis (TTS)\\\\repo_final\\\\pt_etts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primeiro vamos criar um exemplo de metadata pra debuggar com a base do LJSpeech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando o caminho a partir do diretório atual que nos leva a 10 exemplos de arquivos .wav\n",
    "\n",
    "wav_files = ['./data/LJSpeech/LJSpeech-1.1/wavs/' + f for f in os.listdir('./data/LJSpeech/LJSpeech-1.1/wavs')[:10]]\n",
    "wav_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora criando 10 exemplos de textos quaisquer\n",
    "\n",
    "texts = ['hi',\n",
    "        'my name is lucas',\n",
    "        'and yours?',\n",
    "        'my name is jessica',\n",
    "        'how are you doing?',\n",
    "        'fine and you?',\n",
    "        'the same',\n",
    "        'wanna hangout some day?',\n",
    "        'yes of course',\n",
    "        'ok, see ya']\n",
    "\n",
    "speakers = ['rosana',\n",
    "           'marco',\n",
    "           'rosana',\n",
    "           'marco',\n",
    "           'rosana',\n",
    "           'marco',\n",
    "           'rosana',\n",
    "           'marco',\n",
    "           'rosana',\n",
    "           'marco']\n",
    "\n",
    "styles = ['neutral',\n",
    "         'neutral',\n",
    "         'neutral',\n",
    "         'neutral',\n",
    "         'happy',\n",
    "         'happy',\n",
    "         'happy',\n",
    "         'happy',\n",
    "         'happy',\n",
    "         'happy']\n",
    "\n",
    "len(texts), len(speakers), len(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'wav_file':wav_files, 'text': texts, 'speakers': speakers , 'styles': styles})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./experiments/debug/debug_meta.csv\", sep= '|', encoding = 'latin-1', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agora vamos testar o dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from random import randrange\n",
    "from torch.utils.data import DataLoader\n",
    "from TTS.tts.datasets.preprocess import load_meta_data\n",
    "from TTS.tts.datasets.TTSDataset import MyDataset\n",
    "from TTS.tts.layers.losses import TacotronLoss\n",
    "from TTS.tts.utils.distribute import (DistributedSampler,\n",
    "                                      apply_gradient_allreduce,\n",
    "                                      init_distributed, reduce_tensor)\n",
    "from TTS.tts.utils.generic_utils import setup_model, check_config_tts\n",
    "from TTS.tts.utils.io import save_best_model, save_checkpoint\n",
    "from TTS.tts.utils.measures import alignment_diagonal_score\n",
    "from TTS.tts.utils.speakers import (get_speakers, load_speaker_mapping,\n",
    "                                    save_speaker_mapping)\n",
    "from TTS.tts.utils.styles import (get_styles, load_style_mapping,\n",
    "                                    save_style_mapping)\n",
    "from TTS.tts.utils.synthesis import synthesis\n",
    "from TTS.tts.utils.text.symbols import make_symbols, phonemes, symbols\n",
    "from TTS.tts.utils.visual import plot_alignment, plot_spectrogram\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.utils.console_logger import ConsoleLogger\n",
    "from TTS.utils.generic_utils import (KeepAverage, count_parameters,\n",
    "                                     create_experiment_folder, get_git_branch,\n",
    "                                     remove_experiment_folder, set_init_dict)\n",
    "from TTS.utils.io import copy_config_file, load_config\n",
    "from TTS.utils.radam import RAdam\n",
    "from TTS.utils.tensorboard_logger import TensorboardLogger\n",
    "from TTS.utils.training import (NoamLR, adam_weight_decay, check_update,\n",
    "                                gradual_training_scheduler, set_weight_decay,\n",
    "                                setup_torch_training_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda, num_gpus = setup_torch_training_env(True, False)\n",
    "\n",
    "def setup_loader(ap, r, is_val=False, verbose=False, speaker_mapping=None, style_mapping = None):\n",
    "    if is_val and not c.run_eval:\n",
    "        loader = None\n",
    "    else:\n",
    "        dataset = MyDataset(\n",
    "            r,\n",
    "            c.text_cleaner,\n",
    "            compute_linear_spec=c.model.lower() == 'tacotron',\n",
    "            meta_data=meta_data_eval if is_val else meta_data_train,\n",
    "            ap=ap,\n",
    "            tp=c.characters if 'characters' in c.keys() else None,\n",
    "            batch_group_size=0 if is_val else c.batch_group_size *\n",
    "            c.batch_size,\n",
    "            min_seq_len=c.min_seq_len,\n",
    "            max_seq_len=c.max_seq_len,\n",
    "            phoneme_cache_path=c.phoneme_cache_path,\n",
    "            use_phonemes=c.use_phonemes,\n",
    "            phoneme_language=c.phoneme_language,\n",
    "            enable_eos_bos=c.enable_eos_bos_chars,\n",
    "            verbose=verbose,\n",
    "            speaker_mapping=speaker_mapping if c.use_speaker_embedding and c.use_external_speaker_embedding_file else None)\n",
    "        sampler = DistributedSampler(dataset) if num_gpus > 1 else None\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=c.eval_batch_size if is_val else c.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=dataset.collate_fn,\n",
    "            drop_last=False,\n",
    "            sampler=sampler,\n",
    "            num_workers=c.num_val_loader_workers\n",
    "            if is_val else c.num_loader_workers,\n",
    "            pin_memory=False)\n",
    "    return loader\n",
    "\n",
    "def format_data(data, speaker_mapping=None, style_mapping = None):\n",
    "    if speaker_mapping is None and c.use_speaker_embedding and not c.use_external_speaker_embedding_file:\n",
    "        speaker_mapping = load_speaker_mapping(OUT_PATH)\n",
    "    if style_mapping is None and c.use_style_embeddings:\n",
    "        style_mapping = load_style_mapping(OUT_PATH)\n",
    "\n",
    "    # setup input data\n",
    "    text_input = data[0]\n",
    "    text_lengths = data[1]\n",
    "    speaker_names = data[2]\n",
    "    linear_input = data[3] if c.model in [\"Tacotron\"] else None\n",
    "    mel_input = data[4]\n",
    "    mel_lengths = data[5]\n",
    "    stop_targets = data[6]\n",
    "    style_targets = data[10]\n",
    "    avg_text_length = torch.mean(text_lengths.float())\n",
    "    avg_spec_length = torch.mean(mel_lengths.float())\n",
    "\n",
    "    if c.use_speaker_embedding:\n",
    "        if c.use_external_speaker_embedding_file:\n",
    "            speaker_embeddings = data[8]\n",
    "            speaker_ids = None\n",
    "        else:\n",
    "            speaker_ids = [\n",
    "                speaker_mapping[speaker_name] for speaker_name in speaker_names\n",
    "            ]\n",
    "            speaker_ids = torch.LongTensor(speaker_ids)\n",
    "            speaker_embeddings = None\n",
    "    else:\n",
    "        speaker_embeddings = None\n",
    "        speaker_ids = None\n",
    "\n",
    "    if c.use_style_embedding:\n",
    "        style_targets = [\n",
    "                style_mapping[style_target] for style_target in style_targets\n",
    "            ]\n",
    "        if c.use_one_hot_style: # Style target will be a one hotted vector\n",
    "            style_targets_ = np.zeros((len(style_targets), len(style_mapping)))\n",
    "            for i in range(len(style_targets_)):\n",
    "                if(style_targets[i] != 0): # If we force the 0 mapped style to be the non\n",
    "                    style_targets_[i][style_targets[i]] = 1 # For each position we one hot encode it\n",
    "            \n",
    "            style_targets = style_targets_\n",
    "            \n",
    "            style_targets = torch.FloatTensor(style_targets)\n",
    "            \n",
    "            del style_targets_\n",
    "        else: # Style target will be just the indice\n",
    "            style_targets = torch.FloatTensor(style_targets)\n",
    "    else:\n",
    "        style_targets = None\n",
    "\n",
    "    # set stop targets view, we predict a single stop token per iteration.\n",
    "    stop_targets = stop_targets.view(text_input.shape[0],\n",
    "                                     stop_targets.size(1) // c.r, -1)\n",
    "    stop_targets = (stop_targets.sum(2) >\n",
    "                    0.0).unsqueeze(2).float().squeeze(2)\n",
    "\n",
    "    # dispatch data to GPU\n",
    "    if use_cuda:\n",
    "        text_input = text_input.cuda(non_blocking=True)\n",
    "        text_lengths = text_lengths.cuda(non_blocking=True)\n",
    "        mel_input = mel_input.cuda(non_blocking=True)\n",
    "        mel_lengths = mel_lengths.cuda(non_blocking=True)\n",
    "        linear_input = linear_input.cuda(non_blocking=True) if c.model in [\"Tacotron\"] else None\n",
    "        stop_targets = stop_targets.cuda(non_blocking=True)\n",
    "        if speaker_ids is not None:\n",
    "            speaker_ids = speaker_ids.cuda(non_blocking=True)\n",
    "        if speaker_embeddings is not None:\n",
    "            speaker_embeddings = speaker_embeddings.cuda(non_blocking=True)\n",
    "        if style_targets is not None:\n",
    "            style_targets = style_targets.cuda(non_blocking=True)\n",
    "\n",
    "    return text_input, text_lengths, mel_input, mel_lengths, linear_input, stop_targets, speaker_ids, speaker_embeddings, avg_text_length, avg_spec_length, style_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = './experiments/debug/config.json'\n",
    "experiment_folder = './experiments/debug/'\n",
    "c = load_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = experiment_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = AudioProcessor(**c.audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_train, meta_data_eval = load_meta_data(c.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'characters' in c.keys():\n",
    "    symbols, phonemes = make_symbols(**c.characters)\n",
    "\n",
    "# DISTRUBUTED\n",
    "if num_gpus > 1:\n",
    "    init_distributed(args.rank, num_gpus, args.group_id,\n",
    "                     c.distributed[\"backend\"], c.distributed[\"url\"])\n",
    "num_chars = len(phonemes) if c.use_phonemes else len(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_path = None # primeiro ele é none pra criar o json\n",
    "# restore_path = experiment_folder\n",
    "\n",
    "# parse speakers\n",
    "if c.use_speaker_embedding:\n",
    "    speakers = get_speakers(meta_data_train)\n",
    "    if restore_path:\n",
    "        if c.use_external_speaker_embedding_file: # if restore checkpoint and use External Embedding file\n",
    "            prev_out_path = os.path.dirname(args.restore_path)\n",
    "            speaker_mapping = load_speaker_mapping(prev_out_path)\n",
    "            if not speaker_mapping:\n",
    "                print(\"WARNING: speakers.json was not found in restore_path, trying to use CONFIG.external_speaker_embedding_file\")\n",
    "                speaker_mapping = load_speaker_mapping(c.external_speaker_embedding_file)\n",
    "                if not speaker_mapping:\n",
    "                    raise RuntimeError(\"You must copy the file speakers.json to restore_path, or set a valid file in CONFIG.external_speaker_embedding_file\")\n",
    "            speaker_embedding_dim = len(speaker_mapping[list(speaker_mapping.keys())[0]]['embedding'])\n",
    "        elif not c.use_external_speaker_embedding_file: # if restore checkpoint and don't use External Embedding file\n",
    "            prev_out_path = os.path.dirname(restore_path)\n",
    "            speaker_mapping = load_speaker_mapping(prev_out_path)\n",
    "            speaker_embedding_dim = None\n",
    "            assert all([speaker in speaker_mapping\n",
    "                        for speaker in speakers]), \"As of now you, you cannot \" \\\n",
    "                                                \"introduce new speakers to \" \\\n",
    "                                                \"a previously trained model.\"\n",
    "    else: # if start new train and don't use External Embedding file\n",
    "        speaker_mapping = {name: i for i, name in enumerate(speakers)}\n",
    "        speaker_embedding_dim = None\n",
    "    save_speaker_mapping(OUT_PATH, speaker_mapping)\n",
    "    num_speakers = len(speaker_mapping)\n",
    "    print(\"Training with {} speakers: {}\".format(num_speakers,\n",
    "                                                 \", \".join(speakers)))\n",
    "else:\n",
    "    num_speakers = 0\n",
    "    speaker_embedding_dim = None\n",
    "    speaker_mapping = None\n",
    "    \n",
    "    \n",
    "# parse styles\n",
    "if c.use_style_embedding:\n",
    "    styles = get_styles(meta_data_train)\n",
    "    if restore_path:\n",
    "        prev_out_path = os.path.dirname(restore_path)\n",
    "        style_mapping = load_style_mapping(prev_out_path)\n",
    "        style_embedding_dim = None\n",
    "        assert all([style in style_mapping\n",
    "                    for style in styles]), \"As of now you, you cannot \" \\\n",
    "                                            \"introduce new styles to \" \\\n",
    "                                            \"a previously trained model.\"\n",
    "    else: # if start new train and don't use External Embedding file\n",
    "        style_mapping = {name: i for i, name in enumerate(styles)}\n",
    "        style_embedding_dim = None\n",
    "    save_style_mapping(OUT_PATH, style_mapping)\n",
    "    num_styles = len(style_mapping)\n",
    "    print(\"Training with {} styles: {}\".format(num_styles,\n",
    "                                                 \", \".join(styles)))\n",
    "else:\n",
    "    num_styles = 0\n",
    "    style_embedding_dim = None\n",
    "    style_mapping = None\n",
    "\n",
    "model = setup_model(num_chars, num_speakers, c, speaker_embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "data_loader = setup_loader(ap, model.decoder.r, is_val=False,\n",
    "                           verbose=(epoch == 0), speaker_mapping=speaker_mapping, style_mapping = style_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for data in data_loader:\n",
    "    text_input, text_lengths, mel_input, mel_lengths, linear_input, stop_targets, speaker_ids, speaker_embeddings, avg_text_length, avg_spec_length, style_targets = format_data(data, speaker_mapping, style_mapping)\n",
    "    if c.bidirectional_decoder or c.double_decoder_consistency:\n",
    "        decoder_output, postnet_output, alignments, stop_tokens, decoder_backward_output, alignments_backward, logits = model.cuda()(\n",
    "            text_input, text_lengths, mel_input, mel_lengths, speaker_ids=speaker_ids, speaker_embeddings=speaker_embeddings)\n",
    "    else:\n",
    "        decoder_output, postnet_output, alignments, stop_tokens, logits = model.cuda()(\n",
    "            text_input, text_lengths, mel_input, mel_lengths, speaker_ids=speaker_ids, speaker_embeddings=speaker_embeddings)\n",
    "        decoder_backward_output = None\n",
    "        alignments_backward = None\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gst_logits_target = style_targets\n",
    "gst_logits = logits\n",
    "loss = 0\n",
    "return_dict = {}\n",
    "if c.gst_style_loss:\n",
    "    gst_style_loss = functional.binary_cross_entropy(gst_logits.squeeze(0).squeeze(1), gst_logits_target)\n",
    "    loss += gst_style_loss\n",
    "    return_dict['gst_logits_loss'] = gst_style_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-de191f53719d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python TTS/bin/train_tts.py --config_path \"./experiments/debug/config.json\" --experiment_folder \"./experiments/debug/\" --rank 1 --continue_path \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.isfile(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m_audio",
   "language": "python",
   "name": "m_audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
